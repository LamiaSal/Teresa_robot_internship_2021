{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for the training of the Teresa Robot\n",
    "## Importing all necessary tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.gym_envs.RobotEnv import RobotEnv # Training environment\n",
    "import roslibpy # API of ROS\n",
    "from src.robots.Bebop import Bebop # This is the representation of Teresa Robot\n",
    "from src.utils.training_tools import NB_STATES\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "from random import random, randrange, sample\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, LSTM, Lambda\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from src.robots.Teresa import Teresa # This is the representation of Teresa Robot\n",
    "from src.utils.training_tools import NB_STATES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the connection with ROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HOST = 'localhost'\n",
    "PORT = 9090\n",
    "\n",
    "client = roslibpy.Ros(host=HOST, port=PORT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling the Robot Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-28 15:16:38,663     INFO: Connection to ROS MASTER ready.\n",
      "2021-07-28 15:16:38,667     INFO: Connection to ROS MASTER ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): exit\n"
     ]
    }
   ],
   "source": [
    "# client.run()\n",
    "# bebop_controller = Bebop(client)\n",
    "# env = RobotEnv(bebop_controller, client)\n",
    "\n",
    "import time\n",
    "\n",
    "client.run() \n",
    "teresa_controller = Teresa(client) # Robot API\n",
    "env = RobotEnv(teresa_controller, client) # Training Environment\n",
    "#env.takeoff()\n",
    "\n",
    "env.reset()\n",
    "finish = False\n",
    "\n",
    "while not finish:\n",
    "    movement = input('Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): ')\n",
    "    if movement == 'exit':\n",
    "        finish = True\n",
    "        continue\n",
    "    movement = int(movement)\n",
    "    print(movement)\n",
    "    state, reward, done = env.step(movement,1)\n",
    "    #time.sleep(7)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary functions for the training (Neural Network Set up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros( 2*capacity - 1 )\n",
    "        self.data = np.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n",
    "\n",
    "class MemoryBuffer(object):\n",
    "    \"\"\" Memory Buffer Helper class for Experience Replay\n",
    "    using a Sum Tree (for PER)\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prioritized Experience Replay\n",
    "        self.alpha = 0.6\n",
    "        self.epsilon = 0.01\n",
    "        self.buffer = SumTree(buffer_size)\n",
    "\n",
    "        self.count = 0\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def memorize(self, state, action, reward, done, new_state, error=None):\n",
    "        \"\"\" Save an experience to memory, optionally with its TD-Error\n",
    "        \"\"\"\n",
    "\n",
    "        experience = (state, action, reward, done, new_state)\n",
    "        priority = self.priority(error[0])\n",
    "        self.buffer.add(priority, experience)\n",
    "        self.count += 1\n",
    "\n",
    "\n",
    "    def priority(self, error):\n",
    "        \"\"\" Compute an experience priority, as per Schaul et al.\n",
    "        \"\"\"\n",
    "        return (error + self.epsilon) ** self.alpha\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\" Current Buffer Occupation\n",
    "        \"\"\"\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\" Sample a batch, optionally with (PER)\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "\n",
    "    \n",
    "        T = self.buffer.total() // batch_size\n",
    "        for i in range(batch_size):\n",
    "            a, b = T * i, T * (i + 1)\n",
    "            s = np.random.uniform(a, b)\n",
    "            idx, error, data = self.buffer.get(s)\n",
    "            batch.append((*data, idx))\n",
    "        idx = np.array([i[5] for i in batch])\n",
    "\n",
    "        # Return a batch of experience\n",
    "        s_batch = np.array([i[0] for i in batch])\n",
    "        a_batch = np.array([i[1] for i in batch])\n",
    "        r_batch = np.array([i[2] for i in batch])\n",
    "        d_batch = np.array([i[3] for i in batch])\n",
    "        new_s_batch = np.array([i[4] for i in batch])\n",
    "        return s_batch, a_batch, r_batch, d_batch, new_s_batch, idx\n",
    "\n",
    "    def update(self, idx, new_error):\n",
    "        \"\"\" Update priority for idx (PER)\n",
    "        \"\"\"\n",
    "        self.buffer.update(idx, self.priority(new_error))\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\" Clear buffer / Sum Tree\n",
    "        \"\"\"\n",
    "        self.buffer = SumTree(buffer_size)\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(d, k):\n",
    "    \"\"\" Returns a 2D Conv layer, with and ReLU activation\n",
    "    \"\"\"\n",
    "    return Conv2D(d, k, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')\n",
    "\n",
    "def conv_block(inp, d=3, pool_size=(2, 2), k=3):\n",
    "    \"\"\" Returns a 2D Conv block, with a convolutional layer, max-pooling\n",
    "    \"\"\"\n",
    "    conv = conv_layer(d, k)(inp)\n",
    "    return MaxPooling2D(pool_size=pool_size)(conv)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\" Agent Class (Network) for DDQN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, lr, tau):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Initialize Deep Q-Network\n",
    "        self.model = self.network()\n",
    "        self.model.compile(Adam(lr), 'mse')\n",
    "        \n",
    "        # Build target Q-Network\n",
    "        self.target_model = self.network()\n",
    "        self.target_model.compile(Adam(lr), 'mse')\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def huber_loss(self, y_true, y_pred):\n",
    "        return K.mean(K.sqrt(1 + K.square(y_pred - y_true)) - 1, axis=-1)\n",
    "\n",
    "    def network(self):\n",
    "        \"\"\" Build Deep Q-Network\n",
    "        \"\"\"\n",
    "        inp = Input((self.state_dim))\n",
    "\n",
    "        # Determine whether we are dealing with an image input or not\n",
    "        if(len(self.state_dim) > 2):\n",
    "            inp = Input((self.state_dim[1:]))\n",
    "            x = conv_block(inp, 32, (2, 2), 8)\n",
    "            #x = conv_block(x, 64, (2, 2), 4)\n",
    "            x = conv_block(x, 64, (2, 2), 3)\n",
    "            x = Flatten()(x)\n",
    "            x = Dense(16, activation='relu')(x)\n",
    "            #x = Dense(256, activation='relu')(x)\n",
    "        else:\n",
    "            x = Flatten()(inp)\n",
    "            #x = Dense(64, activation='relu')(x)\n",
    "            x = Dense(64, activation='relu')(x)\n",
    "\n",
    "\n",
    "        # Have the network estimate the Advantage function as an intermediate layer\n",
    "        x = Dense(self.action_dim + 1, activation='linear')(x)\n",
    "        x = Lambda(lambda i: K.expand_dims(i[:,0],-1) + i[:,1:] - K.mean(i[:,1:], keepdims=True), output_shape=(self.action_dim,))(x)\n",
    "\n",
    "        return Model(inp, x)\n",
    "\n",
    "    def transfer_weights(self):\n",
    "        \"\"\" Transfer Weights from Model to Target at rate Tau\n",
    "        \"\"\"\n",
    "        W = self.model.get_weights()\n",
    "        tgt_W = self.target_model.get_weights()\n",
    "        for i in range(len(W)):\n",
    "            tgt_W[i] = self.tau * W[i] + (1 - self.tau) * tgt_W[i]\n",
    "        self.target_model.set_weights(tgt_W)\n",
    "\n",
    "    def fit(self, inp, targ):\n",
    "        \"\"\" Perform one epoch of training\n",
    "        \"\"\"\n",
    "        self.model.fit(self.reshape(inp), targ, epochs=1, verbose=0)\n",
    "\n",
    "    def predict(self, inp):\n",
    "        \"\"\" Q-Value Prediction\n",
    "        \"\"\"\n",
    "        inp = np.array(inp)\n",
    "        return self.model.predict(self.reshape(inp))\n",
    "\n",
    "    def target_predict(self, inp):\n",
    "        \"\"\" Q-Value Prediction (using target network)\n",
    "        \"\"\"\n",
    "        inp = np.array(inp)\n",
    "        return self.target_model.predict(self.reshape(inp))\n",
    "\n",
    "    def reshape(self, x):\n",
    "        if len(x.shape) < 4 and len(self.state_dim) > 2: return np.expand_dims(x, axis=0)\n",
    "        elif len(x.shape) < 3: return np.expand_dims(x, axis=0)\n",
    "        else: return x\n",
    "\n",
    "    def Save(self, path):\n",
    "        self.model.save_weights(path + '.h5')\n",
    "        self.model.save(path)\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        #self.model.load_weights(path)\n",
    "        self.model = tf.keras.models.load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DDQN:\n",
    "    \"\"\" Deep Q-Learning Main Algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_dim, state_dim):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        # Environment and DDQN parameters\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.lr = 0.01\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 0.8\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.buffer_size = 20000\n",
    "        \n",
    "        #\n",
    "        if(len(self.state_dim) < 3):\n",
    "            self.tau = 1e-2\n",
    "        else:\n",
    "            self.tau = 1.0\n",
    "            \n",
    "        # Create actor and critic networks\n",
    "        self.agent = Agent(self.state_dim, action_dim, self.lr, self.tau)\n",
    "        \n",
    "        # Memory Buffer for Experience Replay\n",
    "        self.buffer = MemoryBuffer(self.buffer_size)\n",
    "\n",
    "    def policy_action(self, s):\n",
    "        \"\"\" Apply an espilon-greedy policy to pick next action\n",
    "        \"\"\"\n",
    "        if random() <= self.epsilon:\n",
    "            return randrange(self.action_dim)\n",
    "        else:\n",
    "            return np.argmax(self.agent.predict(s)[0])\n",
    "\n",
    "    def action(self, s):\n",
    "        \"\"\" Apply an espilon-greedy policy to pick next action\n",
    "        \"\"\"\n",
    "        return np.argmax(self.agent.predict(s)[0])\n",
    "\n",
    "    def train_agent(self, batch_size):\n",
    "        \"\"\" Train Q-network on batch sampled from the buffer\n",
    "        \"\"\"\n",
    "        # Sample experience from memory buffer with PER\n",
    "        s, a, r, d, new_s, idx = self.buffer.sample_batch(batch_size)\n",
    "\n",
    "        # Apply Bellman Equation on batch samples to train our DDQN\n",
    "        q = self.agent.predict(s)\n",
    "        next_q = self.agent.predict(new_s)\n",
    "        q_targ = self.agent.target_predict(new_s)\n",
    "\n",
    "        for i in range(s.shape[0]):\n",
    "            old_q = q[i, a[i]]\n",
    "            if d[i]:\n",
    "                q[i, a[i]] = r[i]\n",
    "            else:\n",
    "                next_best_action = np.argmax(next_q[i,:])\n",
    "                q[i, a[i]] = r[i] + self.gamma * q_targ[i, next_best_action]\n",
    "           \n",
    "            self.buffer.update(idx[i], abs(old_q - q[i, a[i]]))\n",
    "                \n",
    "        # Train on batch\n",
    "        self.agent.fit(s, q)\n",
    "        # Decay epsilon\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "    def train(self, env, nb_episodes, batch_size):\n",
    "        \"\"\" Main DDQN Training Algorithm\n",
    "        \"\"\"\n",
    "\n",
    "        results = []\n",
    "        for e in range(nb_episodes):\n",
    "            \n",
    "            # Reset episode\n",
    "            time_,cumul_reward, done  = 1,0, False\n",
    "            old_state = env.reset()\n",
    "\n",
    "            while not done:\n",
    "                \n",
    "                # Actor picks an action (following the policy)\n",
    "                a = self.policy_action(old_state)\n",
    "                \n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                #on a changÃ© ici avec quatre actions\n",
    "                #print(\"l et a\", a)\n",
    "                new_state, r, done = env.step(a, time)\n",
    "                \n",
    "                #render the drone image\n",
    "                env.render()\n",
    "                \n",
    "                # Memorize for experience replay\n",
    "                self.memorize(old_state, a, r, done, new_state)\n",
    "                \n",
    "                # Update current state\n",
    "                old_state = new_state\n",
    "                cumul_reward += r\n",
    "                \n",
    "                # Train DDQN and transfer weights to target network\n",
    "                if(self.buffer.size() > batch_size):\n",
    "                    self.train_agent(batch_size)\n",
    "                    self.agent.transfer_weights()\n",
    "                \n",
    "                # Time limit to put the target in the center\n",
    "                time_ += 1\n",
    "                if(time_ >= 10):\n",
    "                    done = True\n",
    "                \n",
    "            results.append(cumul_reward) #to plot the curb at the end\n",
    "                \n",
    "\n",
    "            # Display score\n",
    "            print(str(e)+  \"/\"+str(nb_episodes)+\"  Score: \" + str(cumul_reward))\n",
    "            if (e%20 == 0):self.Save_weights('Model/new_modelo')\n",
    "\n",
    "        return results\n",
    "\n",
    "    def memorize(self, state, action, reward, done, new_state):\n",
    "        \"\"\" Store experience in memory buffer\n",
    "        \"\"\"\n",
    "\n",
    "        q_val = self.agent.predict(state)\n",
    "        q_val_t = self.agent.target_predict(new_state)\n",
    "        next_best_action = np.argmax(self.agent.predict(new_state))\n",
    "        new_val = reward + self.gamma * q_val_t[0, next_best_action]\n",
    "        td_error = abs(new_val - q_val)[0]\n",
    "\n",
    "        self.buffer.memorize(state, action, reward, done, new_state, td_error)\n",
    "\n",
    "    def Save_weights(self, path):\n",
    "        self.agent.Save(path)\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.agent.load_weights(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2  Score: -9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-28 16:26:28,531     INFO: Assets written to: Model/new_modelo/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2  Score: -9\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = 'Final_Model/modelo_new.h5', errno = 2, error message = 'Aucun fichier ou dossier de ce type', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ecc476693caf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-ecc476693caf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Final_Model/modelo_new'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-f76653c78712>\u001b[0m in \u001b[0;36mSave_weights\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mSave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3591555c48d6>\u001b[0m in \u001b[0;36mSave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mSave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite, save_format)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robot_controller/venv/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robot_controller/venv/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = 'Final_Model/modelo_new.h5', errno = 2, error message = 'Aucun fichier ou dossier de ce type', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "   \n",
    "    client.run() # This run the main loop of ROS\n",
    "    teresa_controller = Teresa(client) # Robot API\n",
    "    env = RobotEnv(teresa_controller, client) # Training Environment\n",
    "\n",
    "    #env.reset()\n",
    "    state_dim = (1,3)\n",
    "    action_dim = 4\n",
    "\n",
    "    # Pick algorithm to train\n",
    "    net = DDQN(action_dim, state_dim)\n",
    "    #algo.load_weights('/content/gdrive/MyDrive/PIE/modelo')\n",
    "    #algo.load_weights(args.model_path)\n",
    "\n",
    "    stats = net.train(env, 2, 64)\n",
    "    net.Save_weights('Final_Model/modelo_new')\n",
    "    \n",
    "    return stats\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stats = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(stats)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    client.run() # This run the main loop of ROS\n",
    "    teresa_controller = Teresa(client) # Robot API\n",
    "    env = RobotEnv(teresa_controller, client) # Training Environment\n",
    "\n",
    "    #env.reset()\n",
    "    state_dim = (1,3)\n",
    "    action_dim = 4\n",
    "\n",
    "    # Pick algorithm to train\n",
    "    net = DDQN(action_dim, state_dim)\n",
    "    net.load_weights('Final_Model/new_modelo')\n",
    "    \n",
    "    \n",
    "    env.reset()\n",
    "    old_state, r, done = env.step(0,1)\n",
    "    i = 0\n",
    "    \n",
    "   \n",
    "    while i < 1:\n",
    "        \n",
    "        time_=0\n",
    "        while not done:\n",
    "            a = net.action(old_state)  \n",
    "            old_state, r, done = env.step(a,1)\n",
    "            env.render()\n",
    "            # Time limit to put the target in the center\n",
    "            time_ += 1\n",
    "            if(time_ >= 10):\n",
    "                done = True\n",
    "        print('ok')\n",
    "        while done:\n",
    "            old_state, r, done = env.step(0,1)\n",
    "            env.render()\n",
    "        i += 1\n",
    "          \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
