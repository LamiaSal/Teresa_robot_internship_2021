{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for the training of the Teresa Robot\n",
    "## Importing all necessary tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from src.gym_envs.RobotEnv import RobotEnv # Training environment\n",
    "import roslibpy # API of ROS\n",
    "from src.robots.Bebop import Bebop # This is the representation of Teresa Robot\n",
    "from src.utils.training_tools import NB_STATES\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "from random import random, randrange, sample\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, LSTM, Lambda\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from src.robots.Teresa import Teresa # This is the representation of Teresa Robot\n",
    "from src.utils.training_tools import NB_STATES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the connection with ROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HOST = 'localhost'\n",
    "PORT = 9090\n",
    "\n",
    "client = roslibpy.Ros(host=HOST, port=PORT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling the Robot Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-04 12:41:58,708     INFO: Connection to ROS MASTER ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 1\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 3\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): exit\n"
     ]
    }
   ],
   "source": [
    "client.run()\n",
    "teresa_controller = Teresa(client)\n",
    "env = RobotEnv(teresa_controller, client)\n",
    "\n",
    "env.reset()\n",
    "finish = False\n",
    "\n",
    "while not finish:\n",
    "    movement = input('Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): ')\n",
    "    if movement == 'exit':\n",
    "        finish = True\n",
    "        continue\n",
    "    movement = int(movement)\n",
    "    state, reward, done= env.step(movement,1)\n",
    "    if done and reward:\n",
    "        print(\"Centered\")\n",
    "        #env.reset()\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary functions for the training (Neural Network Set up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros( 2*capacity - 1 )\n",
    "        self.data = np.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n",
    "\n",
    "class MemoryBuffer(object):\n",
    "    \"\"\" Memory Buffer Helper class for Experience Replay\n",
    "    using a Sum Tree (for PER)\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prioritized Experience Replay\n",
    "        self.alpha = 0.6\n",
    "        self.epsilon = 0.01\n",
    "        self.buffer = SumTree(buffer_size)\n",
    "\n",
    "        self.count = 0\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def memorize(self, state, action, reward, done, new_state, error=None):\n",
    "        \"\"\" Save an experience to memory, optionally with its TD-Error\n",
    "        \"\"\"\n",
    "\n",
    "        experience = (state, action, reward, done, new_state)\n",
    "        priority = self.priority(error[0])\n",
    "        self.buffer.add(priority, experience)\n",
    "        self.count += 1\n",
    "\n",
    "\n",
    "    def priority(self, error):\n",
    "        \"\"\" Compute an experience priority, as per Schaul et al.\n",
    "        \"\"\"\n",
    "        return (error + self.epsilon) ** self.alpha\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\" Current Buffer Occupation\n",
    "        \"\"\"\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\" Sample a batch, optionally with (PER)\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "\n",
    "    \n",
    "        T = self.buffer.total() // batch_size\n",
    "        for i in range(batch_size):\n",
    "            a, b = T * i, T * (i + 1)\n",
    "            s = np.random.uniform(a, b)\n",
    "            idx, error, data = self.buffer.get(s)\n",
    "            batch.append((*data, idx))\n",
    "        idx = np.array([i[5] for i in batch])\n",
    "\n",
    "        # Return a batch of experience\n",
    "        s_batch = np.array([i[0] for i in batch])\n",
    "        a_batch = np.array([i[1] for i in batch])\n",
    "        r_batch = np.array([i[2] for i in batch])\n",
    "        d_batch = np.array([i[3] for i in batch])\n",
    "        new_s_batch = np.array([i[4] for i in batch])\n",
    "        return s_batch, a_batch, r_batch, d_batch, new_s_batch, idx\n",
    "\n",
    "    def update(self, idx, new_error):\n",
    "        \"\"\" Update priority for idx (PER)\n",
    "        \"\"\"\n",
    "        self.buffer.update(idx, self.priority(new_error))\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\" Clear buffer / Sum Tree\n",
    "        \"\"\"\n",
    "        self.buffer = SumTree(buffer_size)\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(d, k):\n",
    "    \"\"\" Returns a 2D Conv layer, with and ReLU activation\n",
    "    \"\"\"\n",
    "    return Conv2D(d, k, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')\n",
    "\n",
    "def conv_block(inp, d=3, pool_size=(2, 2), k=3):\n",
    "    \"\"\" Returns a 2D Conv block, with a convolutional layer, max-pooling\n",
    "    \"\"\"\n",
    "    conv = conv_layer(d, k)(inp)\n",
    "    return MaxPooling2D(pool_size=pool_size)(conv)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\" Agent Class (Network) for DDQN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, lr, tau):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Initialize Deep Q-Network\n",
    "        self.model = self.network()\n",
    "        self.model.compile(Adam(lr), 'mse')\n",
    "        \n",
    "        # Build target Q-Network\n",
    "        self.target_model = self.network()\n",
    "        self.target_model.compile(Adam(lr), 'mse')\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def huber_loss(self, y_true, y_pred):\n",
    "        return K.mean(K.sqrt(1 + K.square(y_pred - y_true)) - 1, axis=-1)\n",
    "\n",
    "    def network(self):\n",
    "        \"\"\" Build Deep Q-Network\n",
    "        \"\"\"\n",
    "        inp = Input((self.state_dim))\n",
    "\n",
    "        # Determine whether we are dealing with an image input or not\n",
    "        if(len(self.state_dim) > 2):\n",
    "            inp = Input((self.state_dim[1:]))\n",
    "            x = conv_block(inp, 32, (2, 2), 8)\n",
    "            x = conv_block(x, 64, (2, 2), 4)\n",
    "            x = conv_block(x, 64, (2, 2), 3)\n",
    "            x = Flatten()(x)\n",
    "            x = Dense(256, activation='relu')(x)\n",
    "        else:\n",
    "            x = Flatten()(inp)\n",
    "            x = Dense(64, activation='relu')(x)\n",
    "            x = Dense(64, activation='relu')(x)\n",
    "\n",
    "\n",
    "        # Have the network estimate the Advantage function as an intermediate layer\n",
    "        x = Dense(self.action_dim + 1, activation='linear')(x)\n",
    "        x = Lambda(lambda i: K.expand_dims(i[:,0],-1) + i[:,1:] - K.mean(i[:,1:], keepdims=True), output_shape=(self.action_dim,))(x)\n",
    "\n",
    "        return Model(inp, x)\n",
    "\n",
    "    def transfer_weights(self):\n",
    "        \"\"\" Transfer Weights from Model to Target at rate Tau\n",
    "        \"\"\"\n",
    "        W = self.model.get_weights()\n",
    "        tgt_W = self.target_model.get_weights()\n",
    "        for i in range(len(W)):\n",
    "            tgt_W[i] = self.tau * W[i] + (1 - self.tau) * tgt_W[i]\n",
    "        self.target_model.set_weights(tgt_W)\n",
    "\n",
    "    def fit(self, inp, targ):\n",
    "        \"\"\" Perform one epoch of training\n",
    "        \"\"\"\n",
    "        self.model.fit(self.reshape(inp), targ, epochs=1, verbose=0)\n",
    "\n",
    "    def predict(self, inp):\n",
    "        \"\"\" Q-Value Prediction\n",
    "        \"\"\"\n",
    "        inp = np.array(inp)\n",
    "        return self.model.predict(self.reshape(inp))\n",
    "\n",
    "    def target_predict(self, inp):\n",
    "        \"\"\" Q-Value Prediction (using target network)\n",
    "        \"\"\"\n",
    "        inp = np.array(inp)\n",
    "        return self.target_model.predict(self.reshape(inp))\n",
    "\n",
    "    def reshape(self, x):\n",
    "        if len(x.shape) < 4 and len(self.state_dim) > 2: return np.expand_dims(x, axis=0)\n",
    "        elif len(x.shape) < 3: return np.expand_dims(x, axis=0)\n",
    "        else: return x\n",
    "\n",
    "    def Save(self, path):\n",
    "        self.model.save_weights(path + '.h5')\n",
    "        self.model.save(path)\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        #self.model.load_weights(path)\n",
    "        self.model = tf.keras.models.load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DDQN:\n",
    "    \"\"\" Deep Q-Learning Main Algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_dim, state_dim):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        # Environment and DDQN parameters\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.lr = 0.01\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 0.8\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.buffer_size = 20000\n",
    "        \n",
    "        #\n",
    "        if(len(self.state_dim) < 3):\n",
    "            self.tau = 1e-2\n",
    "        else:\n",
    "            self.tau = 1.0\n",
    "            \n",
    "        # Create actor and critic networks\n",
    "        self.agent = Agent(self.state_dim, action_dim, self.lr, self.tau)\n",
    "        \n",
    "        # Memory Buffer for Experience Replay\n",
    "        self.buffer = MemoryBuffer(self.buffer_size)\n",
    "\n",
    "    def policy_action(self, s):\n",
    "        \"\"\" Apply an espilon-greedy policy to pick next action\n",
    "        \"\"\"\n",
    "        if random() <= self.epsilon:\n",
    "            return randrange(self.action_dim)\n",
    "        else:\n",
    "            return np.argmax(self.agent.predict(s)[0])\n",
    "\n",
    "    def action(self, s):\n",
    "        \"\"\" Apply an espilon-greedy policy to pick next action\n",
    "        \"\"\"\n",
    "        return np.argmax(self.agent.predict(s)[0])\n",
    "\n",
    "    def train_agent(self, batch_size):\n",
    "        \"\"\" Train Q-network on batch sampled from the buffer\n",
    "        \"\"\"\n",
    "        # Sample experience from memory buffer with PER\n",
    "        s, a, r, d, new_s, idx = self.buffer.sample_batch(batch_size)\n",
    "\n",
    "        # Apply Bellman Equation on batch samples to train our DDQN\n",
    "        q = self.agent.predict(s)\n",
    "        next_q = self.agent.predict(new_s)\n",
    "        q_targ = self.agent.target_predict(new_s)\n",
    "\n",
    "        for i in range(s.shape[0]):\n",
    "            old_q = q[i, a[i]]\n",
    "            if d[i]:\n",
    "                q[i, a[i]] = r[i]\n",
    "            else:\n",
    "                next_best_action = np.argmax(next_q[i,:])\n",
    "                q[i, a[i]] = r[i] + self.gamma * q_targ[i, next_best_action]\n",
    "           \n",
    "            self.buffer.update(idx[i], abs(old_q - q[i, a[i]]))\n",
    "                \n",
    "        # Train on batch\n",
    "        self.agent.fit(s, q)\n",
    "        # Decay epsilon\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "    def train(self, env, nb_episodes, batch_size):\n",
    "        \"\"\" Main DDQN Training Algorithm\n",
    "        \"\"\"\n",
    "\n",
    "        results = []\n",
    "        for e in range(nb_episodes):\n",
    "            \n",
    "            # Reset episode\n",
    "            time_,cumul_reward, done  = 1,0, False\n",
    "            old_state = env.reset()\n",
    "\n",
    "            while not done:\n",
    "                \n",
    "                # Actor picks an action (following the policy)\n",
    "                a = self.policy_action(old_state)\n",
    "                \n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                #on a changÃ© ici avec quatre actions\n",
    "                #print(\"l et a\", a)\n",
    "                new_state, r, done = env.step(a, time_)\n",
    "                \n",
    "                #render the drone image\n",
    "                env.render()\n",
    "                \n",
    "                # Memorize for experience replay\n",
    "                self.memorize(old_state, a, r, done, new_state)\n",
    "                \n",
    "                # Update current state\n",
    "                old_state = new_state\n",
    "                cumul_reward += r\n",
    "                \n",
    "                # Train DDQN and transfer weights to target network\n",
    "                if(self.buffer.size() > batch_size):\n",
    "                    self.train_agent(batch_size)\n",
    "                    self.agent.transfer_weights()\n",
    "                \n",
    "                # Time limit to put the target in the center\n",
    "                time_ += 1\n",
    "                if(time_ >= 10):\n",
    "                    done = True\n",
    "                \n",
    "            results.append(cumul_reward) #to plot the curb at the end\n",
    "                \n",
    "\n",
    "            # Display score\n",
    "            print(str(e)+  \"/\"+str(nb_episodes)+\"  Score: \" + str(cumul_reward))\n",
    "            if (e%20 == 0):self.Save_weights('Model/new_modelo')\n",
    "\n",
    "        return results\n",
    "\n",
    "    def memorize(self, state, action, reward, done, new_state):\n",
    "        \"\"\" Store experience in memory buffer\n",
    "        \"\"\"\n",
    "\n",
    "        q_val = self.agent.predict(state)\n",
    "        q_val_t = self.agent.target_predict(new_state)\n",
    "        next_best_action = np.argmax(self.agent.predict(new_state))\n",
    "        new_val = reward + self.gamma * q_val_t[0, next_best_action]\n",
    "        td_error = abs(new_val - q_val)[0]\n",
    "\n",
    "        self.buffer.memorize(state, action, reward, done, new_state, td_error)\n",
    "\n",
    "    def Save_weights(self, path):\n",
    "        self.agent.Save(path)\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.agent.load_weights(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-04 12:42:34,885  WARNING: From /home/saadubuntu/Documents/Teresa_robot_internship_2021/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "2021-08-04 12:42:47,223  WARNING: From /home/saadubuntu/Documents/Teresa_robot_internship_2021/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1  Score: -1.576357142857143\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "   \n",
    "    client.run() # This run the main loop of ROS\n",
    "    teresa_controller = Teresa(client) # Robot API\n",
    "    env = RobotEnv(teresa_controller, client) # Training Environment\n",
    "\n",
    "    #env.reset()\n",
    "    state_dim = (1,3)\n",
    "    action_dim = 4\n",
    "\n",
    "    # Pick algorithm to train\n",
    "    net = DDQN(action_dim, state_dim)\n",
    "    #algo.load_weights('/content/gdrive/MyDrive/PIE/modelo')\n",
    "    #algo.load_weights(args.model_path)\n",
    "\n",
    "    stats = net.train(env, 100, 64) #change the number of episodes here\n",
    "    net.Save_weights('Model/modelo_new')\n",
    "    \n",
    "    return stats\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stats = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(stats)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    client.run() # This run the main loop of ROS\n",
    "    teresa_controller = Teresa(client) # Robot API\n",
    "    env = RobotEnv(teresa_controller, client) # Training Environment\n",
    "\n",
    "    #env.reset()\n",
    "    state_dim = (1,3)\n",
    "    action_dim = 4\n",
    "\n",
    "    # Pick algorithm to train\n",
    "    net = DDQN(action_dim, state_dim)\n",
    "    net.load_weights('Model/modelo_new')\n",
    "    \n",
    "    \n",
    "    env.reset()\n",
    "    old_state, r, done = env.step(0,1)\n",
    "    i = 0\n",
    "    \n",
    "   \n",
    "    while i < 1:\n",
    "        \n",
    "        time_=0\n",
    "        while not done:\n",
    "            a = net.action(old_state)  \n",
    "            old_state, r, done = env.step(a,1)\n",
    "            env.render()\n",
    "            time_ += 1\n",
    "            if(time_ >= 10):\n",
    "                done = True\n",
    "        while done:\n",
    "            old_state, r, done = env.step(0,1)\n",
    "            env.render()\n",
    "        i += 1\n",
    "          \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
