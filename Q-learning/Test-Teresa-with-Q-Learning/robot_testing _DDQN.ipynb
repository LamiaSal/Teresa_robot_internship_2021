{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for the testing on the Teresa Robot\n",
    "## Importing all necessary tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: UserWarning: You do not have a working installation of the service_identity module: 'No module named 'service_identity''.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.\n"
     ]
    }
   ],
   "source": [
    "from src.gym_envs.RobotEnv import RobotEnv # Training environment\n",
    "import roslibpy # API of ROS\n",
    "from src.utils.training_tools import NB_STATES\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "from random import random, randrange, sample\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, LSTM, Lambda\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from src.robots.Teresa_adap import Teresa # This is the representation of Teresa Robot\n",
    "from src.utils.training_tools import NB_STATES\n",
    "\n",
    "from src.robots.actions.camera_adap import DlinkDCSCamera # class for the camera\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the connection with ROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#ip_ordi : 192.168.1.50\n",
    "\n",
    "client = roslibpy.Ros(host=\"192.168.1.14\", port=9090)\n",
    "\n",
    "client.run()\n",
    "print(client.is_connected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the connection with the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DayNightMode': '2',\n",
       " 'LightSensorControl': '3',\n",
       " 'IRLedScheduleSunStart': '00:00',\n",
       " 'IRLedScheduleSunEnd': '00:00',\n",
       " 'IRLedScheduleMonStart': '00:00',\n",
       " 'IRLedScheduleMonEnd': '00:00',\n",
       " 'IRLedScheduleTueStart': '00:00',\n",
       " 'IRLedScheduleTueEnd': '00:00',\n",
       " 'IRLedScheduleWedStart': '00:00',\n",
       " 'IRLedScheduleWedEnd': '00:00',\n",
       " 'IRLedScheduleThuStart': '00:00',\n",
       " 'IRLedScheduleThuEnd': '00:00',\n",
       " 'IRLedScheduleFriStart': '00:00',\n",
       " 'IRLedScheduleFriEnd': '00:00',\n",
       " 'IRLedScheduleSatStart': '00:00',\n",
       " 'IRLedScheduleSatEnd': '00:00'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host='192.168.1.35'\n",
    "user='admin'\n",
    "password='123456'\n",
    "camera = DlinkDCSCamera(host = host, user = user, password = password)\n",
    "camera.set_day_night(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary functions for the training (Neural Network Set up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros( 2*capacity - 1 )\n",
    "        self.data = np.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n",
    "\n",
    "class MemoryBuffer(object):\n",
    "    \"\"\" Memory Buffer Helper class for Experience Replay\n",
    "    using a Sum Tree (for PER)\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prioritized Experience Replay\n",
    "        self.alpha = 0.6\n",
    "        self.epsilon = 0.01\n",
    "        self.buffer = SumTree(buffer_size)\n",
    "\n",
    "        self.count = 0\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def memorize(self, state, action, reward, done, new_state, error=None):\n",
    "        \"\"\" Save an experience to memory, optionally with its TD-Error\n",
    "        \"\"\"\n",
    "\n",
    "        experience = (state, action, reward, done, new_state)\n",
    "        priority = self.priority(error[0])\n",
    "        self.buffer.add(priority, experience)\n",
    "        self.count += 1\n",
    "\n",
    "\n",
    "    def priority(self, error):\n",
    "        \"\"\" Compute an experience priority, as per Schaul et al.\n",
    "        \"\"\"\n",
    "        return (error + self.epsilon) ** self.alpha\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\" Current Buffer Occupation\n",
    "        \"\"\"\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\" Sample a batch, optionally with (PER)\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "\n",
    "    \n",
    "        T = self.buffer.total() // batch_size\n",
    "        for i in range(batch_size):\n",
    "            a, b = T * i, T * (i + 1)\n",
    "            s = np.random.uniform(a, b)\n",
    "            idx, error, data = self.buffer.get(s)\n",
    "            batch.append((*data, idx))\n",
    "        idx = np.array([i[5] for i in batch])\n",
    "\n",
    "        # Return a batch of experience\n",
    "        s_batch = np.array([i[0] for i in batch])\n",
    "        a_batch = np.array([i[1] for i in batch])\n",
    "        r_batch = np.array([i[2] for i in batch])\n",
    "        d_batch = np.array([i[3] for i in batch])\n",
    "        new_s_batch = np.array([i[4] for i in batch])\n",
    "        return s_batch, a_batch, r_batch, d_batch, new_s_batch, idx\n",
    "\n",
    "    def update(self, idx, new_error):\n",
    "        \"\"\" Update priority for idx (PER)\n",
    "        \"\"\"\n",
    "        self.buffer.update(idx, self.priority(new_error))\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\" Clear buffer / Sum Tree\n",
    "        \"\"\"\n",
    "        self.buffer = SumTree(buffer_size)\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(d, k):\n",
    "    \"\"\" Returns a 2D Conv layer, with and ReLU activation\n",
    "    \"\"\"\n",
    "    return Conv2D(d, k, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')\n",
    "\n",
    "def conv_block(inp, d=3, pool_size=(2, 2), k=3):\n",
    "    \"\"\" Returns a 2D Conv block, with a convolutional layer, max-pooling\n",
    "    \"\"\"\n",
    "    conv = conv_layer(d, k)(inp)\n",
    "    return MaxPooling2D(pool_size=pool_size)(conv)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\" Agent Class (Network) for DDQN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, lr, tau):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Initialize Deep Q-Network\n",
    "        self.model = self.network()\n",
    "        self.model.compile(Adam(lr), 'mse')\n",
    "        \n",
    "        # Build target Q-Network\n",
    "        self.target_model = self.network()\n",
    "        self.target_model.compile(Adam(lr), 'mse')\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def huber_loss(self, y_true, y_pred):\n",
    "        return K.mean(K.sqrt(1 + K.square(y_pred - y_true)) - 1, axis=-1)\n",
    "\n",
    "    def network(self):\n",
    "        \"\"\" Build Deep Q-Network\n",
    "        \"\"\"\n",
    "        inp = Input((self.state_dim))\n",
    "\n",
    "        # Determine whether we are dealing with an image input or not\n",
    "        if(len(self.state_dim) > 2):\n",
    "            inp = Input((self.state_dim[1:]))\n",
    "            x = conv_block(inp, 32, (2, 2), 8)\n",
    "            x = conv_block(x, 64, (2, 2), 4)\n",
    "            x = conv_block(x, 64, (2, 2), 3)\n",
    "            x = Flatten()(x)\n",
    "            x = Dense(256, activation='relu')(x)\n",
    "        else:\n",
    "            x = Flatten()(inp)\n",
    "            x = Dense(64, activation='relu')(x)\n",
    "            x = Dense(64, activation='relu')(x)\n",
    "\n",
    "\n",
    "        # Have the network estimate the Advantage function as an intermediate layer\n",
    "        x = Dense(self.action_dim + 1, activation='linear')(x)\n",
    "        x = Lambda(lambda i: K.expand_dims(i[:,0],-1) + i[:,1:] - K.mean(i[:,1:], keepdims=True), output_shape=(self.action_dim,))(x)\n",
    "\n",
    "        return Model(inp, x)\n",
    "\n",
    "    def transfer_weights(self):\n",
    "        \"\"\" Transfer Weights from Model to Target at rate Tau\n",
    "        \"\"\"\n",
    "        W = self.model.get_weights()\n",
    "        tgt_W = self.target_model.get_weights()\n",
    "        for i in range(len(W)):\n",
    "            tgt_W[i] = self.tau * W[i] + (1 - self.tau) * tgt_W[i]\n",
    "        self.target_model.set_weights(tgt_W)\n",
    "\n",
    "    def fit(self, inp, targ):\n",
    "        \"\"\" Perform one epoch of training\n",
    "        \"\"\"\n",
    "        self.model.fit(self.reshape(inp), targ, epochs=1, verbose=0)\n",
    "\n",
    "    def predict(self, inp):\n",
    "        \"\"\" Q-Value Prediction\n",
    "        \"\"\"\n",
    "        inp = np.array(inp)\n",
    "        return self.model.predict(self.reshape(inp))\n",
    "\n",
    "    def target_predict(self, inp):\n",
    "        \"\"\" Q-Value Prediction (using target network)\n",
    "        \"\"\"\n",
    "        inp = np.array(inp)\n",
    "        return self.target_model.predict(self.reshape(inp))\n",
    "\n",
    "    def reshape(self, x):\n",
    "        if len(x.shape) < 4 and len(self.state_dim) > 2: return np.expand_dims(x, axis=0)\n",
    "        elif len(x.shape) < 3: return np.expand_dims(x, axis=0)\n",
    "        else: return x\n",
    "\n",
    "    def Save(self, path):\n",
    "        self.model.save_weights(path + '.h5')\n",
    "        self.model.save(path)\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        #self.model.load_weights(path)\n",
    "        self.model = tf.keras.models.load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DDQN:\n",
    "    \"\"\" Deep Q-Learning Main Algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_dim, state_dim):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        # Environment and DDQN parameters\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.lr = 0.01\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 0.8\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.buffer_size = 20000\n",
    "        \n",
    "        #\n",
    "        if(len(self.state_dim) < 3):\n",
    "            self.tau = 1e-2\n",
    "        else:\n",
    "            self.tau = 1.0\n",
    "            \n",
    "        # Create actor and critic networks\n",
    "        self.agent = Agent(self.state_dim, action_dim, self.lr, self.tau)\n",
    "        \n",
    "        # Memory Buffer for Experience Replay\n",
    "        self.buffer = MemoryBuffer(self.buffer_size)\n",
    "\n",
    "    def policy_action(self, s):\n",
    "        \"\"\" Apply an espilon-greedy policy to pick next action\n",
    "        \"\"\"\n",
    "        if random() <= self.epsilon:\n",
    "            return randrange(self.action_dim)\n",
    "        else:\n",
    "            return np.argmax(self.agent.predict(s)[0])\n",
    "\n",
    "    def action(self, s):\n",
    "        \"\"\" Apply an espilon-greedy policy to pick next action\n",
    "        \"\"\"\n",
    "        return np.argmax(self.agent.predict(s)[0])\n",
    "\n",
    "    def train_agent(self, batch_size):\n",
    "        \"\"\" Train Q-network on batch sampled from the buffer\n",
    "        \"\"\"\n",
    "        # Sample experience from memory buffer with PER\n",
    "        s, a, r, d, new_s, idx = self.buffer.sample_batch(batch_size)\n",
    "\n",
    "        # Apply Bellman Equation on batch samples to train our DDQN\n",
    "        q = self.agent.predict(s)\n",
    "        next_q = self.agent.predict(new_s)\n",
    "        q_targ = self.agent.target_predict(new_s)\n",
    "\n",
    "        for i in range(s.shape[0]):\n",
    "            old_q = q[i, a[i]]\n",
    "            if d[i]:\n",
    "                q[i, a[i]] = r[i]\n",
    "            else:\n",
    "                next_best_action = np.argmax(next_q[i,:])\n",
    "                q[i, a[i]] = r[i] + self.gamma * q_targ[i, next_best_action]\n",
    "           \n",
    "            self.buffer.update(idx[i], abs(old_q - q[i, a[i]]))\n",
    "                \n",
    "        # Train on batch\n",
    "        self.agent.fit(s, q)\n",
    "        # Decay epsilon\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "    def train(self, env, nb_episodes, batch_size):\n",
    "        \"\"\" Main DDQN Training Algorithm\n",
    "        \"\"\"\n",
    "\n",
    "        results = []\n",
    "        for e in range(nb_episodes):\n",
    "            \n",
    "            # Reset episode\n",
    "            time,cumul_reward, done  = 1,0, False\n",
    "            old_state = env.reset()\n",
    "\n",
    "            while not done:\n",
    "                \n",
    "                # Actor picks an action (following the policy)\n",
    "                a = self.policy_action(old_state)\n",
    "                \n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                #on a changÃ© ici avec quatre actions\n",
    "                #print(\"l et a\", a)\n",
    "                new_state, r, done = env.step(a, time)\n",
    "                \n",
    "                #render the drone image\n",
    "                env.render()\n",
    "                \n",
    "                # Memorize for experience replay\n",
    "                self.memorize(old_state, a, r, done, new_state)\n",
    "                \n",
    "                # Update current state\n",
    "                old_state = new_state\n",
    "                cumul_reward += r\n",
    "                \n",
    "                # Train DDQN and transfer weights to target network\n",
    "                if(self.buffer.size() > batch_size):\n",
    "                    self.train_agent(batch_size)\n",
    "                    self.agent.transfer_weights()\n",
    "                \n",
    "                # Time limit to put the target in the center\n",
    "                time += 1\n",
    "                if(time >= 10):\n",
    "                    done = True\n",
    "                    \n",
    "                \n",
    "\n",
    "            # Display score\n",
    "            print(str(e)+  \"/\"+str(nb_episodes)+\"  Score: \" + str(cumul_reward))\n",
    "            if (e%20 == 0):self.Save_weights('Model/new_modelo')\n",
    "\n",
    "        return results\n",
    "\n",
    "    def memorize(self, state, action, reward, done, new_state):\n",
    "        \"\"\" Store experience in memory buffer\n",
    "        \"\"\"\n",
    "\n",
    "        q_val = self.agent.predict(state)\n",
    "        q_val_t = self.agent.target_predict(new_state)\n",
    "        next_best_action = np.argmax(self.agent.predict(new_state))\n",
    "        new_val = reward + self.gamma * q_val_t[0, next_best_action]\n",
    "        td_error = abs(new_val - q_val)[0]\n",
    "\n",
    "        self.buffer.memorize(state, action, reward, done, new_state, td_error)\n",
    "\n",
    "    def Save_weights(self, path):\n",
    "        self.agent.Save(path)\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.agent.load_weights(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angular': {'y': 0.2, 'x': 0.2, 'z': 0.2}}\n",
      "{'linear': {'x': -0.5}}\n",
      "{'linear': {'x': -0.5}}\n",
      "{'linear': {'x': -0.5}}\n",
      "{'linear': {'x': 0.5}}\n",
      "{'angular': {'y': 0.2, 'x': 0.2, 'z': 0.2}}\n",
      "{'linear': {'x': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    client.run() # This run the main loop of ROS\n",
    "    teresa_controller = Teresa(client) # Robot API\n",
    "    env = RobotEnv(teresa_controller, client) # Training Environment\n",
    "\n",
    "    #env.reset()\n",
    "    state_dim = (1,3)\n",
    "    action_dim = 4\n",
    "\n",
    "    # Pick algorithm to train\n",
    "    net = DDQN(action_dim, state_dim)\n",
    "    net.load_weights('Final_Model/new_modelo')\n",
    "    \n",
    "    env.reset()\n",
    "    old_state, r, done = env.step(0,1)\n",
    "    i = 0\n",
    "    \n",
    "   \n",
    "    while i < 1:\n",
    "        time_=0\n",
    "        while not done:\n",
    "            a = net.action(old_state) #changed\n",
    "            old_state, r, done = env.step(a,1)\n",
    "            env.render()\n",
    "            env.render()\n",
    "            # Time limit to put the target in the center\n",
    "            time_ += 1\n",
    "            if(time_ >= 10):\n",
    "                done = True\n",
    "        while done:\n",
    "            old_state, r, done = env.step(0,1)\n",
    "            env.render()\n",
    "        i += 1\n",
    "          \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
