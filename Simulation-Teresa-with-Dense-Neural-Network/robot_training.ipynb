{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for the training of the Teresa Robot\n",
    "## Importing all necessary tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: UserWarning: You do not have a working installation of the service_identity module: 'No module named 'service_identity''.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-26 16:13:28,896  WARNING: From /home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from src.gym_envs.RobotEnv import RobotEnv # Training environment\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import roslibpy # API of ROS\n",
    "from src.robots.Teresa import Teresa # This is the representation of Teresa Robot\n",
    "from src.utils.training_tools import NB_STATES\n",
    "\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the connection with ROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HOST = 'localhost'\n",
    "PORT = 9090\n",
    "\n",
    "client = roslibpy.Ros(host=HOST, port=PORT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating representation of the robot and introducing it in the Gym Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-26 16:13:29,038     INFO: Connection to ROS MASTER ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2382\n",
      "Body detected and centered\n",
      "0\n",
      "2383\n",
      "Body detected and centered\n",
      "0\n",
      "0\n",
      "0\n",
      "2499\n",
      "Body detected and centered\n",
      "2376\n",
      "Body detected and centered\n",
      "0\n",
      "2372\n",
      "Body detected and centered\n",
      "2264\n",
      "Body detected and centered\n",
      "2380\n",
      "Body detected and centered\n",
      "3198\n",
      "0\n",
      "2387\n",
      "Body detected and centered\n",
      "2626\n",
      "Body detected and centered\n",
      "2388\n",
      "Body detected and centered\n",
      "2274\n",
      "Body detected and centered\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# client.connect()\n",
    "client.run() # This run the main loop of ROS\n",
    "teresa_controller = Teresa(client) # Robot API\n",
    "env = RobotEnv(teresa_controller, client) # Training Environment\n",
    "\n",
    "env.reset() # Restarting the environment to the initial state\n",
    "import time\n",
    "for i in range(20):\n",
    "    state, reward, done, _ = env.step(np.random.randint(4))\n",
    "    print(state)\n",
    "    env.render()\n",
    "    if done and reward:\n",
    "        print(\"Body detected and centered\")\n",
    "        #env.reset()\n",
    "    elif done:\n",
    "        print(\"Face not detected. End of the episode\")\n",
    "        #env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling the Robot Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 2\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 2\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 1\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 0\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 0\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 0\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 0\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 0\n",
      "Centered\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 0\n",
      "Centered\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 0\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 0\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 0\n",
      "Centered\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 1\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 2\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 1\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 1\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 0\n",
      "Centered\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 2\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 1\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 1\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 1\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 1\n",
      "Centered\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 1\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 1\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 3\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 1\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 0\n",
      "Centered\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 0\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 0\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 0\n",
      "Centered\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): 0\n",
      "Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): exit\n"
     ]
    }
   ],
   "source": [
    "client.run()\n",
    "teresa_controller = Teresa(client)\n",
    "env = RobotEnv(teresa_controller, client)\n",
    "\n",
    "env.reset()\n",
    "finish = False\n",
    "\n",
    "while not finish:\n",
    "    movement = input('Enter a movement (0 Right, 1 Left, 2 Backward, 3 Forward, exit): ')\n",
    "    if movement == 'exit':\n",
    "        finish = True\n",
    "        continue\n",
    "    movement = int(movement)\n",
    "    state, reward, done, _ = env.step(movement)\n",
    "    if done and reward:\n",
    "        print(\"Centered\")\n",
    "        #env.reset()\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the filter haarcascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('Haarcascades/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('Haarcascades/haarcascade_eye.xml')\n",
    "body_cascade = cv2.CascadeClassifier('Haarcascades/haarcascade_fullbody.xml')\n",
    "\n",
    "img = cv2.imread('env_observation')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "faces = face_cascade.detectMultiScale(gray, 1.2, 3)\n",
    "body = body_cascade.detectMultiScale(gray,1.2,3)\n",
    "\n",
    "if len(body)>0:\n",
    "    print(\"body detected\")\n",
    "    x, y, w, h = body[0]\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (25, 125, 225), 5)\n",
    "\n",
    "print(\"ok\")\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "    for (ex,ey,ew,eh) in eyes:\n",
    "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "print('ok')\n",
    "\n",
    "cv2.imshow('img',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary functions for the training (Neural Network Set up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-ed69ceeb7041>:53: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-26 16:14:34,171  WARNING: From <ipython-input-7-ed69ceeb7041>:53: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-26 16:14:34,206  WARNING: From /home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "state_size =  NB_STATES # 800 is the image size this maybe variable\n",
    "action_size = 4\n",
    "new_graph = tf.Graph()\n",
    "## TRAINING Hyperparameters\n",
    "\n",
    "initializer=tf.initializers.glorot_uniform()\n",
    "\n",
    "def discount_correct_rewards(r, gamma=0.99):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(range(0, r.size)):\n",
    "    #if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "\n",
    "  discounted_r -= discounted_r.mean()\n",
    "  discounted_r /- discounted_r.std()\n",
    "  return discounted_r\n",
    "\n",
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    #print(\"len episode rewards\",episode_rewards)\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        #print(\"dans boucle\",episode_rewards[i],\"cyl\",cumulative)\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    if std :\n",
    "        discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    else:\n",
    "        discounted_episode=[]\n",
    "        discounted_episode_rewards[0] = np.array(mean)\n",
    "        print(\"ATTTTTTTTTTTTTTTTTT\")\n",
    "    #print(\"dis\",discounted_episode_rewards,\"std\",std)\n",
    "    \n",
    "    return discounted_episode_rewards\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    input_ = tf.placeholder(tf.float32, [None, state_size], name=\"input_\")\n",
    "    actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "    discounted_episode_rewards_ = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards\")\n",
    "    \n",
    "    # Add this placeholder for having this variable in tensorboard\n",
    "    mean_reward_ = tf.placeholder(tf.float32 , name=\"mean_reward\")\n",
    "\n",
    "    with tf.name_scope(\"fc1\"):\n",
    "        fc1 = tf.layers.dense(input_ , 20, activation=tf.nn.relu,kernel_initializer=initializer)\n",
    "\n",
    "    with tf.name_scope(\"fc2\"):\n",
    "        fc2 = tf.layers.dense(fc1, action_size,activation= tf.nn.relu, kernel_initializer=initializer)\n",
    "    \n",
    "    with tf.name_scope(\"fc3\"):\n",
    "        fc3 = tf.layers.dense(fc2, action_size, activation= None,kernel_initializer=initializer)\n",
    "\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        action_distribution = tf.nn.softmax(fc3)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
    "        # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
    "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = fc3, labels = actions)\n",
    "        #loss = tf.nn.sparse_softmax_cross_entropy_with_logits (neg_log_prob * discounted_episode_rewards_)\n",
    "        loss = tf.reduce_mean(neg_log_prob * discounted_episode_rewards_) \n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "# Setup TensorBoard Writer\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "\n",
    "## Reward mean\n",
    "tf.summary.scalar(\"Reward_mean\", mean_reward_)\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all() #procedure d'affichage groupée dans tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3596\n",
      "[[0.25069818 0.24877483 0.2518319  0.24869514]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  0 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 1.0\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.25091654 0.24450013 0.26015297 0.24443042]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  2 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.5\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.2500287  0.247163   0.25679266 0.24601565]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  4 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.4\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.253955   0.238918   0.26895535 0.23817164]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  6 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.33333333333333337\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.25545165 0.23627448 0.273248   0.23502587]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  7 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3541666666666667\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.2559951  0.2333299  0.27615508 0.23451991]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  8 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3703703703703704\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.25512853 0.24129751 0.26392552 0.23964842]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  10 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.33333333333333337\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.25394946 0.23042156 0.27841854 0.23721051]] action proba 0.0 length 13 rew 0\n",
      "==========================================\n",
      "Episode:  11 length 13\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.3055555555555555\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.25304696 0.22919464 0.27930412 0.23845421]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  12 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3012820512820512\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.2521782  0.22760813 0.2814134  0.23880024]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  15 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.26562499999999994\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.25129583 0.22627907 0.28300574 0.23941933]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  16 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.30882352941176466\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.25204602 0.22437072 0.28434744 0.23923582]] action proba 0.05555555555555555 length 18 rew 1\n",
      "==========================================\n",
      "Episode:  17 length 18\n",
      "Reward:  0.05555555555555555\n",
      "Mean Reward val 0.294753086419753\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.24953298 0.22999185 0.2737027  0.24677247]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  18 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.2967836257309941\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.2538689  0.22078155 0.2866726  0.23867689]] action proba 0.07692307692307693 length 13 rew 1\n",
      "==========================================\n",
      "Episode:  19 length 13\n",
      "Reward:  0.07692307692307693\n",
      "Mean Reward val 0.2857905982905982\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.254775   0.21996643 0.28714776 0.23811089]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  20 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.2959910459910459\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.25503242 0.21881433 0.28897405 0.23717913]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  21 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.327991452991453\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.25626707 0.21718296 0.2907437  0.23580633]] action proba 0.08333333333333333 length 12 rew 1\n",
      "==========================================\n",
      "Episode:  23 length 12\n",
      "Reward:  0.08333333333333333\n",
      "Mean Reward val 0.3041310541310541\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.25751686 0.21562473 0.29240087 0.2344575 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  24 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.31196581196581197\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.25829858 0.21514829 0.29169673 0.23485643]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  25 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3127876397107166\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.25859958 0.21503855 0.29053798 0.23582391]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  28 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.28535640604606116\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.25905085 0.21478377 0.28960824 0.23655716]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  29 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.29251119251119245\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.25909132 0.2160652  0.28697962 0.23786387]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  30 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.2992043798495411\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.2586675  0.21690229 0.28570738 0.23872288]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  31 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.30027090964590963\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.25793102 0.21787572 0.28418607 0.24000722]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  33 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.2973137973137973\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.25652093 0.22022313 0.28064483 0.24261107]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  34 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.30310483167626023\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.2556546  0.2208499  0.27934867 0.2441469 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  36 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3002343002343002\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.25670427 0.2170143  0.28503418 0.24124727]] action proba 0.0 length 4 rew 0\n",
      "==========================================\n",
      "Episode:  37 length 4\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.29233339759655547\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.25641954 0.21631964 0.28591892 0.241342  ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  38 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3104786950940797\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.25571236 0.21594766 0.2856098  0.2427302 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  39 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3152167277167277\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.25510383 0.21552882 0.28545132 0.24391605]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  42 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.31648067694579324\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.25442    0.21534847 0.2848841  0.24534748]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  43 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.332015207015207\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.25275007 0.21801738 0.28109452 0.24813804]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  44 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.3278116944783611\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.2521287  0.21780111 0.28148448 0.24858575]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  46 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.33513885641545216\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.25166786 0.2171735  0.2826036  0.24855502]] action proba 0.07692307692307693 length 13 rew 1\n",
      "==========================================\n",
      "Episode:  47 length 13\n",
      "Reward:  0.07692307692307693\n",
      "Mean Reward val 0.329759361009361\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.249998   0.21952622 0.27892077 0.25155506]] action proba 0.0 length 8 rew 0\n",
      "==========================================\n",
      "Episode:  48 length 8\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.3230295781316189\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.24905664 0.22023782 0.27776673 0.25293878]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  49 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3215689865689865\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.24754581 0.2216472  0.27576366 0.25504324]] action proba 0.0625 length 16 rew 1\n",
      "==========================================\n",
      "Episode:  50 length 16\n",
      "Reward:  0.0625\n",
      "Mean Reward val 0.3164892025186143\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.24704558 0.22077695 0.2774444  0.25473312]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  52 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3139801760084779\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.24606873 0.2214078  0.27698404 0.25553942]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  53 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.31742498756387644\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.24538997 0.22089304 0.27795485 0.25576222]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  56 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3182622689201636\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.24470852 0.2204543  0.27870303 0.25613415]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  57 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.32139567807671254\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.24408764 0.22005448 0.27938488 0.256473  ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  58 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.33289744624490386\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.24349126 0.21890114 0.2816404  0.25596726]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  61 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3221658493835913\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.24098352 0.22332358 0.27506557 0.26062736]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  62 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.3196976083880846\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.24221829 0.21730782 0.2851723  0.2553015 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  63 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.33032733325702074\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.24123397 0.21697462 0.28565013 0.2561413 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  64 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3406299896684512\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.23877236 0.22065267 0.28017506 0.2603999 ]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  67 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.3274404313007254\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.23934938 0.21504742 0.2895274  0.25607586]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  68 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.32994129461520766\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.23705642 0.21973336 0.2833114  0.2598988 ]] action proba 0.05 length 20 rew 1\n",
      "==========================================\n",
      "Episode:  70 length 20\n",
      "Reward:  0.05\n",
      "Mean Reward val 0.3213513989922441\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.23468156 0.22533093 0.27546543 0.26452208]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  71 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3238326295617962\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.23713048 0.2154808  0.2929196  0.2544691 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  73 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.32859390984390985\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.23584978 0.21813564 0.29003713 0.25597748]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  75 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.32257828063749117\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.23558594 0.2168816  0.29328164 0.25425076]] action proba 0.1 length 10 rew 1\n",
      "==========================================\n",
      "Episode:  76 length 10\n",
      "Reward:  0.1\n",
      "Mean Reward val 0.31968765361622503\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.23504977 0.21646632 0.2951898  0.25329408]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  79 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3201993666056166\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.23452143 0.2160016  0.29712984 0.2523471 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  80 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.32859196701789295\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.23353319 0.21643017 0.29677173 0.25326487]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  81 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3286497885583251\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.23222144 0.21806777 0.29493308 0.25477767]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  83 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3327295554974126\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.23098029 0.21971051 0.29276302 0.2565461 ]] action proba 0.1 length 10 rew 1\n",
      "==========================================\n",
      "Episode:  84 length 10\n",
      "Reward:  0.1\n",
      "Mean Reward val 0.32999156072685487\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.23011309 0.22100241 0.29096237 0.25792208]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  86 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.32527911105497315\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.2293688  0.22176027 0.29007342 0.25879747]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  87 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.3238554847929848\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22836728 0.22339818 0.28826606 0.25996846]] action proba 0.1 length 10 rew 1\n",
      "==========================================\n",
      "Episode:  88 length 10\n",
      "Reward:  0.1\n",
      "Mean Reward val 0.3213402546267715\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.22733982 0.225029   0.2861549  0.2614763 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  90 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.32526684243717213\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.2252003  0.23154995 0.27628782 0.26696193]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  91 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.327166115888942\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22474127 0.23193316 0.27655688 0.26676872]] action proba 0.1 length 10 rew 1\n",
      "==========================================\n",
      "Episode:  92 length 10\n",
      "Reward:  0.1\n",
      "Mean Reward val 0.32472346948153397\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22416717 0.2326706  0.27640927 0.266753  ]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  93 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3239285389551347\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22336769 0.23433761 0.27414963 0.26814508]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  94 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3310450806503438\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.22260694 0.23481365 0.27412817 0.26845127]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  96 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3345286872348728\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22188658 0.23517068 0.27426875 0.26867408]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  97 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3345164897460816\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.2217296  0.23549104 0.27415523 0.26862413]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  98 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.33618804035470706\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.22109756 0.23577453 0.2740438  0.26908404]] action proba 0.0 length 6 rew 0\n",
      "==========================================\n",
      "Episode:  99 length 6\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.33282615995115994\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.22052553 0.23603131 0.27394214 0.26950106]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  100 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.334481346486297\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.22001082 0.23575231 0.27435398 0.2698829 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  102 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3328409319914174\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.219467   0.23547922 0.2748664  0.2701873 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  103 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.33284566661970505\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.2189732  0.23522979 0.27508894 0.270708  ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  104 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3344376126518983\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.21853459 0.2350125  0.27579275 0.2706601 ]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  105 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3336410314004653\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.21813704 0.23525825 0.27613896 0.27046573]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  106 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.333638155717595\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.21754768 0.23524393 0.27691492 0.27029353]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  107 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3398081727942839\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.21679911 0.23498993 0.27733836 0.27087253]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  109 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.33666014541014544\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.21609513 0.23472206 0.27785063 0.27133214]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  110 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.34263618013618014\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.21525553 0.2342508  0.27804536 0.27244836]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  111 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.34850549995639285\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.2143088  0.23457947 0.27795225 0.27315947]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  113 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.34385335668230405\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.2141861  0.23478086 0.27806392 0.27296907]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  115 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.34223519536019537\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.21421269 0.23485374 0.27842999 0.27250358]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  116 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3478571167673732\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.21458302 0.23456553 0.27987167 0.2709798 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  117 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3477340338569152\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.21605633 0.23386723 0.28125295 0.26882347]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  118 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.34586231928668904\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.2163634  0.23338632 0.28156555 0.2686847 ]] action proba 0.0 length 8 rew 0\n",
      "==========================================\n",
      "Episode:  119 length 8\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.3429801332926333\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.22032659 0.23207504 0.28372478 0.26387364]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  120 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.34427781814145453\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22285654 0.23196296 0.2849582  0.26022223]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  121 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3496525901239016\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.2251424  0.23144178 0.28753302 0.25588286]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  122 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3549399674399675\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.22460614 0.23104954 0.28835025 0.25599408]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  123 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.3532296220804286\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22377066 0.23113835 0.28892922 0.25616178]] action proba 0.1111111111111111 length 9 rew 1\n",
      "==========================================\n",
      "Episode:  124 length 9\n",
      "Reward:  0.1111111111111111\n",
      "Mean Reward val 0.35129267399267405\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.21693707 0.23285623 0.28581396 0.26439264]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  125 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.3496384237455667\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22259066 0.23181525 0.28954452 0.25604957]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  126 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3495100372068877\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.22207268 0.23140152 0.28952974 0.25699598]] action proba 0.0 length 16 rew 0\n",
      "==========================================\n",
      "Episode:  128 length 16\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.34409127694011427\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.21845672 0.23177193 0.2875469  0.26222447]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  130 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.342654768895227\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.2174052  0.231824   0.28710833 0.26366255]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  131 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3438467782217782\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.22073972 0.23085603 0.28946152 0.25894266]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  133 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3461774233229457\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.22044595 0.23072226 0.2896816  0.2591502 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  136 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3410299858292559\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22018538 0.23055193 0.29006076 0.25920194]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  137 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.34037034825078305\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.22068289 0.23030719 0.2902542  0.25875577]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  139 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3378888670852957\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.2209153  0.23006594 0.29021928 0.25879952]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  140 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3390385914322085\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.22112663 0.22984669 0.29018667 0.25884005]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  142 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3412897999436461\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.22116521 0.2295857  0.2899632  0.2592859 ]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  144 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.3377317797145383\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tooooooooo long\n",
      "3596\n",
      "[[0.21792524 0.23043719 0.2877439  0.2638936 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  146 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3354043632104856\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.21732248 0.23057988 0.2869869  0.26511073]] action proba 0.1 length 10 rew 1\n",
      "==========================================\n",
      "Episode:  148 length 10\n",
      "Reward:  0.1\n",
      "Mean Reward val 0.33157343216068047\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.22101718 0.22958846 0.2886424  0.2607519 ]] action proba 0.07142857142857142 length 14 rew 1\n",
      "==========================================\n",
      "Episode:  150 length 14\n",
      "Reward:  0.07142857142857142\n",
      "Mean Reward val 0.32765476796933746\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22237816 0.22912598 0.28910106 0.2593948 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  151 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3287886181800655\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22330269 0.22873938 0.29017282 0.2577852 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  152 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.32881832220067514\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.2233345  0.22851425 0.2909789  0.25717232]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  153 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3331766447837876\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.21777974 0.22956379 0.28792518 0.26473126]] action proba 0.1 length 10 rew 1\n",
      "==========================================\n",
      "Episode:  154 length 10\n",
      "Reward:  0.1\n",
      "Mean Reward val 0.33167227933356963\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22124079 0.22825894 0.2905476  0.25995263]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  155 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3359564313891237\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.22020617 0.22785403 0.29155186 0.26038796]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  158 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3311899578409012\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.21927606 0.22740263 0.2926342  0.26068714]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  160 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3332869769981571\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.21823993 0.22697404 0.29343966 0.26134637]] action proba 0.07692307692307693 length 13 rew 1\n",
      "==========================================\n",
      "Episode:  161 length 13\n",
      "Reward:  0.07692307692307693\n",
      "Mean Reward val 0.33170448378781714\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.21732496 0.22645484 0.29452816 0.26169205]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  162 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3358044562799164\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.21577547 0.22645319 0.293287   0.26448435]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  163 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.3349763803269901\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.21935648 0.22451168 0.2959921  0.26013973]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  164 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3359765234765235\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22048582 0.22363907 0.29653326 0.2593418 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  165 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3369646167085926\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.22111161 0.22306801 0.29589868 0.2599217 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  167 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.33890551412872844\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22148968 0.22253995 0.29512122 0.2608492 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  168 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3398587359386176\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22262372 0.22203052 0.29377827 0.2615674 ]] action proba 0.0 length 10 rew 0\n",
      "==========================================\n",
      "Episode:  169 length 10\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.3378595669036845\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.22364703 0.22157042 0.2925703  0.26221234]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  170 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.33880775657091444\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22428772 0.22201338 0.29059812 0.26310074]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  171 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3397449207768975\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22511256 0.22159053 0.2897243  0.26357257]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  172 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3397078595778018\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22556902 0.22086997 0.28962758 0.26393348]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  173 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3435026419940213\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.22754598 0.21967994 0.28948268 0.26329148]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  174 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3434445316588174\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22907576 0.21843211 0.28884783 0.26364434]] action proba 0.0 length 18 rew 0\n",
      "==========================================\n",
      "Episode:  175 length 18\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.3414931422743923\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.22269236 0.22097574 0.28411832 0.27221355]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  176 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.341447041658906\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.23060842 0.21653731 0.28752652 0.26532775]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  177 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3423377886158785\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.23099858 0.2150748  0.2876825  0.26624408]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  178 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3460118791822702\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.23093545 0.21354075 0.28871483 0.26680902]] action proba 0.09090909090909091 length 11 rew 1\n",
      "==========================================\n",
      "Episode:  179 length 11\n",
      "Reward:  0.09090909090909091\n",
      "Mean Reward val 0.3445946414696414\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.23189247 0.21166232 0.2899863  0.2664589 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  180 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3482156655499197\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.23157796 0.21017313 0.29028687 0.2679621 ]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  181 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.34740129376118384\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.23103277 0.20870294 0.29096928 0.26929507]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  182 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35096740691002987\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.22976543 0.2074463  0.29092464 0.2718636 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  183 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.35449475795943186\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.2265934  0.20805435 0.28869557 0.27665663]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  184 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.35438037188037186\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.231526   0.20392756 0.29093325 0.27361313]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  185 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.35516327310682144\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2292791  0.20461613 0.2890843  0.27702048]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  186 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.355046535460974\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.23479764 0.20029294 0.29134044 0.27356902]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  187 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.358477138995756\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.23592138 0.1985411  0.29097068 0.27456686]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  188 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3592259372021277\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.2357642  0.1973295  0.29037756 0.2765287 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  189 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3625984322694849\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.23494178 0.19631676 0.2892659  0.27947554]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  190 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3624452118562067\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.23358268 0.19568744 0.28780055 0.2829294 ]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  191 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.3613015239968365\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.23273493 0.1949567  0.286491   0.28581735]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  192 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.36461084252535025\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.2314145  0.1943448  0.28483692 0.28940383]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  193 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3678860443680031\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.23161663 0.19320022 0.28327146 0.29191166]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  194 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3711276543968851\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.2317025  0.1918068  0.28281313 0.29367757]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  195 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3743361867724112\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.23127492 0.19056585 0.28195637 0.2962029 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  196 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3775121452151909\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.23080036 0.18910404 0.28208658 0.29800898]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  197 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.37632701894065534\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.23026353 0.18771106 0.28247356 0.2995518 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  198 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.37946105402135555\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.227761   0.18787694 0.28275308 0.30160892]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  200 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.37734369693324915\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22942734 0.18460827 0.284734   0.3012304 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  201 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.37712582384612087\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.22894515 0.18322901 0.28621647 0.30160937]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  203 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3750624987757341\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22837214 0.18202135 0.28745094 0.30215558]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  204 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37567195000121834\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22700104 0.18123253 0.28837898 0.30338743]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  205 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37627548422451335\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.22496419 0.18081227 0.2890202  0.30520338]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  206 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3756654577306751\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.22283992 0.18092443 0.28947896 0.30675673]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  210 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37091350592535427\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.22091013 0.18103042 0.2898147  0.30824476]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  212 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3721255856819237\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.2191861  0.18102364 0.2903102  0.30948   ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  214 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3702143399236422\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.21765144 0.18088886 0.29094112 0.31051853]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  216 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.36910637365706495\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.21617325 0.18080704 0.29139966 0.31162003]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  217 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.36970680313570226\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.21500175 0.18056187 0.29247025 0.31196618]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  219 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3686185594708323\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.21384753 0.18036474 0.29333267 0.31245512]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  220 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3714754890659869\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.2139286  0.17825125 0.29602408 0.3117961 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  221 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37205442830442836\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.21223053 0.1792094  0.29634616 0.3122139 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  222 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3726281752627045\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.21048397 0.18041083 0.29638824 0.312717  ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  223 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37319679948028167\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.20950934 0.18068565 0.29696992 0.31283504]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  224 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.37598259148259155\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.20834176 0.18088037 0.29705453 0.31372333]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  227 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.372497440425072\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.20726088 0.18104458 0.2970782  0.31461638]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  228 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37305422016120704\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.20617504 0.1818741  0.29697514 0.31497565]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  230 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3719888156576468\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.20522499 0.18248276 0.29709482 0.3151975 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  233 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3714932325509248\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.20433593 0.18302004 0.29714286 0.31550112]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  236 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.3676346684258077\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.20350343 0.18348409 0.2971272  0.31588534]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  237 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3674905451691166\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.2023316  0.18469013 0.29660523 0.316373  ]] action proba 0.0 length 15 rew 0\n",
      "==========================================\n",
      "Episode:  238 length 15\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.3659529278253128\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.20128776 0.1857745  0.29613265 0.31680503]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  240 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.36499066286410686\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.20029944 0.18675226 0.29558283 0.31736547]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  241 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3676146683894617\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.19963637 0.18774217 0.29526    0.31736144]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  242 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3681594639927973\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.19847468 0.18985242 0.29375356 0.31791934]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  244 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3665146248309514\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.19801761 0.19066377 0.29332638 0.31799224]] action proba 0.08333333333333333 length 12 rew 1\n",
      "==========================================\n",
      "Episode:  245 length 12\n",
      "Reward:  0.08333333333333333\n",
      "Mean Reward val 0.36536348136957897\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.19759604 0.191271   0.29318285 0.31795013]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  246 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3679328599875159\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.19737707 0.1910872  0.2941664  0.31736934]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  247 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3677933457671361\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.19679616 0.19156869 0.29457438 0.31706077]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  248 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.37033232831425605\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.1964118  0.1913094  0.2960212  0.31625754]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  253 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.36501082578838484\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.19605425 0.19103092 0.29742858 0.31548625]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  255 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.36411230371191305\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.19568685 0.19074292 0.29880208 0.3147682 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  256 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3639925411812571\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.19556548 0.19034278 0.3003311  0.31376064]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  258 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.36214703893275324\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.19498947 0.19072227 0.30125418 0.31303412]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  259 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.36267724262916573\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.1945264  0.1914451  0.30164054 0.31238797]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  261 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.3606720728381034\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.19402723 0.19235015 0.30185544 0.3117671 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  262 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3631029775041182\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.19352889 0.19277616 0.3029093  0.31078568]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  264 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.36224937012672864\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.19300555 0.19317344 0.30375543 0.3100655 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  265 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3627672296375304\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.19222893 0.19329543 0.30516672 0.3093089 ]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  267 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.36080628016262345\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.19225939 0.19230834 0.30724883 0.3081834 ]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  268 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.3600845715622667\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.19068609 0.1944882  0.3067418  0.3080839 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  269 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3624546287046287\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.18978608 0.19501801 0.30692995 0.30826598]] action proba 0.1111111111111111 length 9 rew 1\n",
      "==========================================\n",
      "Episode:  270 length 9\n",
      "Reward:  0.1111111111111111\n",
      "Mean Reward val 0.3615271618500401\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.18922022 0.19539519 0.30709162 0.30829296]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  271 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.36387448846088555\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.18849435 0.19563927 0.3068185  0.30904788]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  272 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.36620461853978337\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.1876295  0.19576362 0.30615944 0.31044742]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  274 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.36717767585949407\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.18694682 0.19558851 0.30566657 0.3117982 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  275 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3676589161643509\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.18635517 0.19490615 0.30577198 0.31296673]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  276 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3675349970927589\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.18601412 0.19409117 0.30586836 0.3140264 ]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  277 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.36693235321832446\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.18571612 0.19362938 0.30586687 0.31478766]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  278 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3674092981888681\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.18544704 0.193213   0.3058646  0.31547534]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  279 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3696685506953364\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.18504423 0.19266072 0.30655673 0.31573823]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  281 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3679333127471425\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.1846434  0.19213048 0.3073119  0.31591427]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  283 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3688633598404725\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18425237 0.19161214 0.30792114 0.31621435]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  284 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.37107787436734807\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.18373555 0.1909645  0.3081211  0.31717893]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  286 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3696534060210019\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.18322061 0.19035275 0.30819613 0.31823054]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  287 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.37184210947231777\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.18259509 0.18962432 0.30791536 0.3198652 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  288 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3740156661869464\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.18187182 0.1887931  0.30731615 0.32201895]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  290 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37316332483858256\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.18120645 0.18797152 0.306692   0.32413003]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  294 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3697983984000933\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.18069468 0.18720944 0.30609235 0.32600346]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  295 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.36967520561270556\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.18003474 0.18651068 0.3057755  0.3276791 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  297 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.36831273219696037\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.17942049 0.18582915 0.30541193 0.3293385 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  298 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3687531578417866\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.17845404 0.18518618 0.30503458 0.33132514]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  300 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.36962522988270496\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.17754388 0.1845354  0.3045894  0.33333132]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  301 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3700569344195172\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.17632316 0.1839201  0.30465952 0.33509725]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  302 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3721359544379346\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.17507356 0.18318947 0.30436677 0.33737022]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  303 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3717341914299151\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.17373043 0.18276662 0.30408666 0.33941627]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  304 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.3709252268678498\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.17249584 0.18258792 0.3037426  0.34117362]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  311 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.36367156258983185\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.17144455 0.18241546 0.3034072  0.34273282]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  312 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.36290903363587074\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.17045438 0.18216816 0.30293596 0.34444153]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  313 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3649379857580495\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.16943073 0.18178275 0.3032159  0.34557068]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  314 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3653667540572303\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.16851082 0.18143404 0.30346578 0.3465894 ]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  315 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.3648434415443909\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.16751887 0.18101442 0.30365443 0.34781227]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  316 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3668470899937777\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.16648129 0.18045796 0.3034615  0.34959933]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  317 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.36883813688058975\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.16542211 0.17979887 0.30398658 0.35079253]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  318 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.3683088637242243\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.16479142 0.17923215 0.30440262 0.35157377]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  319 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.37028289852508606\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.16408016 0.17854595 0.3044145  0.3529594 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  320 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3722446340436995\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.16334972 0.17863302 0.30415905 0.35385826]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  321 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3741941848696507\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.16255082 0.17853858 0.30356666 0.3553439 ]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  324 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.371509315470854\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.16181876 0.17842859 0.30313227 0.35662034]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  325 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3734372010062194\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.16107757 0.17900671 0.3024883  0.35742742]] action proba 0.05263157894736842 length 19 rew 1\n",
      "==========================================\n",
      "Episode:  326 length 19\n",
      "Reward:  0.05263157894736842\n",
      "Mean Reward val 0.3724561440580272\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.16055332 0.1795102  0.3018611  0.35807538]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  327 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3728449972773625\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.16008106 0.17996475 0.30129492 0.3586592 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  328 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3727249010343715\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.15964682 0.18017738 0.30101004 0.3591657 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  330 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.37147983617414365\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.15924832 0.18047833 0.30072522 0.35954815]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  331 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3733729691977155\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.15877163 0.18059956 0.30117154 0.3594573 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  332 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.37525473205297766\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.15822722 0.18056023 0.30226693 0.35894555]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  333 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3751292188831583\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15754095 0.18030025 0.30319628 0.35896248]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  334 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.37699450479693997\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.15681067 0.17991833 0.30472106 0.35854995]] action proba 0.058823529411764705 length 17 rew 1\n",
      "==========================================\n",
      "Episode:  335 length 17\n",
      "Reward:  0.058823529411764705\n",
      "Mean Reward val 0.37604756737019834\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.15612133 0.17954801 0.3062614  0.35806924]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  336 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3756735389803758\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1553868  0.17919697 0.30747867 0.35793757]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  337 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3760413687467061\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.15472595 0.17887942 0.3085785  0.35781604]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  338 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.37788195467960667\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.15402016 0.17844538 0.31025767 0.3572768 ]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  340 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.37625214849380256\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.15335572 0.17804545 0.3119179  0.35668096]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  342 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37551598436264333\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1527494  0.17765084 0.3135309  0.35606882]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  343 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.375393360377093\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.15204768 0.17767398 0.3147522  0.3555262 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  344 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.37527144725522704\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.15139887 0.1774873  0.3155702  0.3555436 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  345 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37563193440188825\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.15084371 0.17774299 0.3163681  0.35504517]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  346 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.3749096521701825\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.15024132 0.17796537 0.31732333 0.35446995]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  350 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.37158684511791074\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.14968383 0.17814426 0.3182678  0.3539041 ]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  351 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.37100468552003785\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.14906538 0.17822695 0.31900692 0.35370073]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  354 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.37068633606493895\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.14850093 0.17828518 0.31979698 0.3534169 ]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  356 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.36930994202535944\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.14795402 0.17830488 0.32065108 0.35309005]] action proba 0.09090909090909091 length 11 rew 1\n",
      "==========================================\n",
      "Episode:  357 length 11\n",
      "Reward:  0.09090909090909091\n",
      "Mean Reward val 0.3685322860166548\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1474378  0.17824782 0.3215981  0.35271624]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  358 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3702912490082518\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.14686365 0.17804158 0.3231498  0.35194498]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  359 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3701885881313771\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.14634384 0.17785105 0.32429013 0.35151502]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  361 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3709057782521982\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.14600775 0.17765637 0.32526076 0.35107508]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  362 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3712614097170682\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.14573073 0.1779076  0.32619575 0.3501659 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  364 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37059696363642675\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.14544086 0.17808647 0.32698172 0.3494909 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  365 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3709505238450704\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1451794  0.17824736 0.3276912  0.34888202]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  366 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.3704847186029857\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.14486884 0.17838253 0.32820353 0.34854504]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  368 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3698316848978205\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.14456561 0.17848416 0.32859185 0.34835845]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  370 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.36822304277669243\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.14426836 0.17871553 0.328881   0.34813514]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  371 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3699213679305185\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.14388986 0.17876324 0.32984832 0.3474986 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  372 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3716105867832517\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.14343208 0.1786366  0.33030647 0.3476249 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  374 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3709619969870744\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.14314269 0.17850181 0.33068582 0.34766966]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  375 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3726349703993428\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.14276737 0.1782132  0.3306149  0.34840453]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  376 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3723096787006708\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.14231676 0.1778163  0.33051187 0.34935507]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  379 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37068618123724445\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1419039  0.17757832 0.3303471  0.3501707 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  380 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37102558758570314\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1411726  0.17732021 0.33011705 0.3513901 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  381 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3726721174611332\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.14040531 0.1769219  0.32949716 0.3531757 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  382 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3743100492693287\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tooooooooo long\n",
      "3596\n",
      "[[0.13961    0.17640238 0.32964504 0.35434252]] action proba 0.1111111111111111 length 9 rew 1\n",
      "==========================================\n",
      "Episode:  384 length 9\n",
      "Reward:  0.1111111111111111\n",
      "Mean Reward val 0.37265418176951687\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.13889895 0.17608415 0.32970747 0.35530946]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  385 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37298409321570986\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.13863073 0.17543153 0.33005333 0.35588443]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  387 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3719206013262818\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.13770106 0.17648953 0.32876614 0.35704324]] action proba 0.0 length 9 rew 0\n",
      "==========================================\n",
      "Episode:  388 length 9\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.37096450723546875\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.13717915 0.17673121 0.32831928 0.35777038]] action proba 0.0 length 12 rew 0\n",
      "==========================================\n",
      "Episode:  389 length 12\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.37001331619127525\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.13670963 0.17694879 0.32791552 0.35842606]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  390 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.36970637676367607\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.13619775 0.1773783  0.32739884 0.35902518]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  391 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.36961358838757824\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.13573717 0.17776579 0.32693222 0.35956484]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  392 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.36952127221695674\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1353201  0.17811196 0.3262452  0.36032277]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  393 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3692179187341726\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.13494043 0.1783017  0.32593456 0.3608233 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  395 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3698784342961212\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.13457242 0.17842177 0.32556942 0.3614364 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  396 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3714656422701864\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.13413325 0.17835553 0.32482874 0.3626825 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  397 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.37304487432478395\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.13364221 0.1781367  0.32487655 0.36334455]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  398 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3746161904292331\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.13310714 0.17778343 0.3256264  0.363483  ]] action proba 0.0 length 12 rew 0\n",
      "==========================================\n",
      "Episode:  399 length 12\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.37367964995316\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.13262592 0.17746474 0.32630283 0.36360648]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  400 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.37524154608794014\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.13210127 0.1770233  0.32761484 0.36326054]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  401 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.37679567159518407\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.13152355 0.17645267 0.32838514 0.3636387 ]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  402 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.3762742596722845\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.13094349 0.17580071 0.32896367 0.36429206]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  403 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3778181352671551\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.13033167 0.17506236 0.3301837  0.3644222 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  406 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37626173623570186\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12975356 0.17437385 0.33122075 0.3646519 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  407 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.37779050649002616\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.12982565 0.1736846  0.33192292 0.36456677]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  408 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37808930720765443\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12988466 0.17305733 0.33199304 0.36506495]] action proba 0.0 length 20 rew 0\n",
      "==========================================\n",
      "Episode:  409 length 20\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.37716713816568453\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.12993754 0.17249316 0.33205542 0.36551383]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  410 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.37868254658863915\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.12988374 0.1718206  0.3316923  0.3666033 ]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  411 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.3782488510872104\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12976466 0.17112502 0.3316681  0.36744225]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  412 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37854364805794355\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12965682 0.17049949 0.33164486 0.36819884]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  413 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3800447503573205\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.12947103 0.16979156 0.33232412 0.36841333]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  415 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3806214582882949\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12928912 0.16912858 0.33310747 0.36847478]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  416 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38210677853220787\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.1290235  0.16837153 0.33339047 0.36921456]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  417 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3823888197318916\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.1288036  0.16810887 0.33314955 0.36993796]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  419 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3829488729712635\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12856826 0.16782482 0.33309352 0.37051338]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  420 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.38263307992382584\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.12835209 0.16756347 0.33271784 0.3713666 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  422 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3831880062598834\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12813452 0.16728246 0.33253264 0.37205032]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  423 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.38267734272310694\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.12785593 0.1673281  0.33230543 0.3725105 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  425 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.383228153320651\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tooooooooo long\n",
      "3596\n",
      "[[0.12760457 0.16748375 0.33204624 0.37286544]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  427 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3826055918565358\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.12732634 0.16761927 0.3319763  0.3730782 ]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  430 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.38040648100834645\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12705386 0.16773145 0.33184117 0.3733735 ]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  431 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.3798565982811446\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12676345 0.16777435 0.3315662  0.37389594]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  432 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3812888001326893\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.12640117 0.16765049 0.33089378 0.37505454]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  433 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3815623282429827\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12609415 0.16795029 0.32979745 0.3761581 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  434 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.38145145699031685\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12566063 0.16820072 0.3290283  0.37711024]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  435 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3813410943213788\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.12527038 0.16842596 0.32833466 0.37796894]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  437 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38188291580849576\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12490099 0.16859667 0.32760346 0.37889883]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  438 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3832909273897976\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.12446813 0.16858403 0.326526   0.38042188]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  440 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.3818360932519754\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.12418254 0.16855119 0.32552692 0.38173938]] action proba 0.0 length 19 rew 0\n",
      "==========================================\n",
      "Episode:  442 length 19\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.3801122282711538\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.12392471 0.16852039 0.3246264  0.38292846]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  444 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3795274542115082\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.12370104 0.16865164 0.32374263 0.38390478]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  447 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37810204715205614\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.12358858 0.16876674 0.32292557 0.3847191 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  449 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3771623343498989\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12347156 0.16884975 0.32210922 0.38556948]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  450 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.37854334912961085\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.12326448 0.16875383 0.32096046 0.38702124]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  451 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3799182532244568\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.12299978 0.16852854 0.3206443  0.38782737]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  452 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.38018333434316665\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12277435 0.16834313 0.32092184 0.38796073]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  453 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.38044724770364424\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.12258396 0.16819331 0.32173154 0.3874912 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  455 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38097160188038265\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.12239411 0.1680254  0.32236928 0.3872112 ]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  457 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3798538219595076\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12233832 0.16786112 0.32291764 0.38688293]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  458 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3801155783386808\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12228788 0.16771294 0.32341248 0.38658673]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  459 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3814631531683793\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.12216572 0.16744283 0.32456625 0.38582522]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  460 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.38172028298797067\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12433159 0.16101477 0.3321694  0.38248426]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  461 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.3812547989699592\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12130743 0.16680585 0.3261605  0.38572624]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  462 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.3807013328814712\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12091041 0.1665022  0.32695973 0.38562775]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  463 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3809584420778473\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12055314 0.16622832 0.32768092 0.38553762]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  464 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.38085602248914946\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.12021489 0.16559123 0.3287922  0.38540167]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  465 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38218465763402254\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.119871   0.16571005 0.32952395 0.384895  ]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  466 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.3816721790156566\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.11957406 0.1660562  0.3299647  0.38440496]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  467 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3819250162399821\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.1193165  0.1663822  0.33092964 0.38337177]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  470 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3816155150749716\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.11907586 0.16663812 0.33192223 0.38236383]] action proba 0.09090909090909091 length 11 rew 1\n",
      "==========================================\n",
      "Episode:  471 length 11\n",
      "Reward:  0.09090909090909091\n",
      "Mean Reward val 0.38099961163394214\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.11881715 0.16683528 0.33270168 0.38164592]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  472 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3812511980786907\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.11825572 0.16695672 0.33329314 0.38149437]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  473 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3815017229772589\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11772383 0.16664186 0.33374783 0.38188648]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  474 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3814003158411664\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.11709262 0.16633177 0.33410457 0.38247108]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  475 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3816494748415001\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.11621568 0.16600698 0.33488393 0.38289344]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  476 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3813734801353334\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.11523919 0.16555892 0.33549574 0.3837062 ]] action proba 0.09090909090909091 length 11 rew 1\n",
      "==========================================\n",
      "Episode:  477 length 11\n",
      "Reward:  0.09090909090909091\n",
      "Mean Reward val 0.38076581404908605\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.11437685 0.16536954 0.33592036 0.3843332 ]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  478 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.38049281652497524\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.113594   0.16507314 0.3361102  0.38522267]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  482 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3794121306738367\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.11287668 0.16478048 0.33622804 0.3861148 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  483 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.37931692654710014\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.11209001 0.16449389 0.33654985 0.38686627]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  484 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38059668546143605\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.11131766 0.1641006  0.3375526  0.38702917]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  485 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3808423712938199\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.11033086 0.1636981  0.3389102  0.38706088]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  486 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38211374219465394\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10936655 0.1631766  0.33968252 0.3877743 ]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  488 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.3808065285251461\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10849946 0.162664   0.3402797  0.38855684]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  490 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.37993426839537636\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10783798 0.162192   0.34076032 0.38920972]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  491 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38119456459782475\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10718203 0.16163523 0.34190488 0.3892778 ]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  492 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.3807594167318386\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10649908 0.16105177 0.34279802 0.38965115]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  494 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.38023109585615444\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10588328 0.16068669 0.3434688  0.3899612 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  495 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3804725654209606\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10533066 0.16035746 0.3440729  0.39023897]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  499 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3794287848975929\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10482737 0.16005014 0.34469914 0.39042342]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  500 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3796694460055817\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10466611 0.15982299 0.34480035 0.39071056]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  501 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3799091483043754\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10449284 0.1592057  0.3447989  0.3915025 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  502 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.37981655225075506\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10419189 0.15843931 0.3452471  0.3921217 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  503 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3800550114724797\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10420696 0.15780056 0.34518984 0.39280266]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  507 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37804670429553106\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10420831 0.15720129 0.34508085 0.39350945]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  508 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3792686164678385\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10413374 0.1565135  0.34451544 0.3948374 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  511 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.37802290191822224\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10414244 0.15589762 0.34396243 0.39599743]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  512 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3792353329086351\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10464966 0.15532038 0.3432741  0.39675596]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  513 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.380443046268735\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10503223 0.15465318 0.34218958 0.39812502]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  514 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38164606947986374\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10530082 0.15390629 0.34075272 0.40004018]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  515 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.38122944273022574\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10553292 0.15316708 0.3391803  0.40211976]] action proba 0.07142857142857142 length 14 rew 1\n",
      "==========================================\n",
      "Episode:  516 length 14\n",
      "Reward:  0.07142857142857142\n",
      "Mean Reward val 0.3806302147393134\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10570857 0.1524835  0.33797657 0.40383127]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  517 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3808606583402028\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1061243  0.15153381 0.33692524 0.4054166 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  518 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38205360504860314\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10641947 0.15053764 0.33552262 0.4075202 ]] action proba 0.058823529411764705 length 17 rew 1\n",
      "==========================================\n",
      "Episode:  519 length 17\n",
      "Reward:  0.058823529411764705\n",
      "Mean Reward val 0.38143200874930155\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10666959 0.1496323  0.33443612 0.40926203]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  520 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.38117974001849675\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10687745 0.14869554 0.3330874  0.41133967]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  522 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3806780966532252\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10704669 0.14783077 0.33200732 0.41311526]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  523 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38186000868251296\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.1071417  0.14694932 0.3317589  0.41415006]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  524 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3830374181897844\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10717104 0.14605631 0.3322533  0.41451937]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  528 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3806136948008257\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10719623 0.14533414 0.33267802 0.4147916 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  530 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3801217411480919\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1072122  0.14464337 0.33320725 0.41493714]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  531 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3812869258451819\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10721371 0.14466313 0.33345094 0.41467226]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  533 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38173154410044347\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10719895 0.14465967 0.33358473 0.41455662]] action proba 0.1 length 10 rew 1\n",
      "==========================================\n",
      "Episode:  534 length 10\n",
      "Reward:  0.1\n",
      "Mean Reward val 0.3812049430834333\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10714044 0.14461932 0.33356145 0.4146788 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  535 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3814265756523074\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10680309 0.14454067 0.3339886  0.4146677 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  536 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3816473827739978\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10649974 0.14446957 0.33437356 0.41465703]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  537 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.38186736905136953\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10651138 0.14444804 0.3342711  0.41476944]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  538 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.38208653905312956\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10648539 0.14404671 0.3340644  0.41540352]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  541 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38181668736095353\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10646069 0.14367141 0.33399412 0.41587386]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  542 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.382955146500252\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10635555 0.14319967 0.33347413 0.41697064]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  543 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.38286392993193047\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10624297 0.1425969  0.3329499  0.41821015]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  544 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38399628969352323\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10605843 0.14192277 0.33202314 0.41999567]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  545 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3851245016171615\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10588312 0.14193833 0.33096856 0.42121002]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  546 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3862485884514994\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10564189 0.14182056 0.3295703  0.42296726]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  547 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.38645616402001853\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1056817  0.14140402 0.3283332  0.42458108]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  548 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3866629833933883\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10597114 0.14072463 0.32724    0.4260643 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  549 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3868690506963094\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10623155 0.14011347 0.32625404 0.42740095]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  550 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3867718896847613\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10622306 0.13985203 0.32534534 0.42857963]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  554 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3857861463356819\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10620124 0.13960266 0.32447323 0.42972296]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  555 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3859915669357976\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10616377 0.13935494 0.323105   0.43137625]] action proba 0.0625 length 16 rew 1\n",
      "==========================================\n",
      "Episode:  556 length 16\n",
      "Reward:  0.0625\n",
      "Mean Reward val 0.3854107921298088\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10611168 0.13907836 0.321777   0.43303296]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  557 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.38507851472455823\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10608884 0.13904788 0.3206574  0.43420586]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  558 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3852840987769293\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10606761 0.13901956 0.31964967 0.4352632 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  559 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3851913295529229\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10605694 0.13900515 0.31901914 0.43591872]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  561 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3855999013338734\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1060307  0.13913152 0.31843713 0.43640068]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  562 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38669119813434605\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10591979 0.13911365 0.31747642 0.43749017]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  564 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3862073354860829\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10579912 0.13907744 0.3165746  0.43854886]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  565 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.38587834726084247\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10592542 0.13909055 0.31576318 0.4392209 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  567 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.386280184066262\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10603008 0.13909079 0.315183   0.43969616]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  570 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3860020044652134\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10611616 0.13907063 0.31460732 0.44020587]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  571 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3862013016602042\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10617308 0.1390253  0.31350848 0.44129306]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  573 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3857267326648725\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10620768 0.13896942 0.31241906 0.44240385]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  574 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3867950339993684\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10619143 0.13883966 0.31217486 0.44279408]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  575 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3878596259542306\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10617385 0.1393229  0.31178197 0.44272116]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  578 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38757710630334513\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10613752 0.13973652 0.31136054 0.4427654 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  580 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3871035190183078\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1060883  0.14011508 0.31117603 0.44262064]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  581 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38815660575538974\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10599718 0.14037848 0.31173888 0.4418854 ]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  582 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.38791963044534616\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10608928 0.14065331 0.3123293  0.4409281 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  583 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.38811154888636445\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10612929 0.14053077 0.31273547 0.44060448]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  584 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.38778999068313985\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10633824 0.1404481  0.31295642 0.4402573 ]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  585 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.38741264712679774\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10652062 0.14036535 0.31298256 0.44013146]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  586 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.388456237165764\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10659559 0.14015669 0.31257826 0.44066954]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  587 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3894962775787474\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.1065736  0.13983539 0.31178898 0.44180205]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  588 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.38940092453249037\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10657327 0.13957195 0.31162325 0.4422316 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  590 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.38864717069876503\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10656386 0.13931231 0.31160885 0.44251496]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  591 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3896798612888009\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10646576 0.13894635 0.31117237 0.44341555]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  592 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.38986589862220933\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1066371  0.13831784 0.31079456 0.4442505 ]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  593 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.38963043414641435\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10677895 0.13773729 0.3101208  0.4453629 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  594 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39065626534952963\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10681617 0.13708569 0.3090931  0.44700506]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  595 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.39083972799156064\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10682575 0.13617153 0.3086129  0.44838974]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  596 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3907434023723676\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10681218 0.13518438 0.3081166  0.4498868 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  597 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39176222611421985\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10675336 0.13423096 0.308396   0.45061958]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  599 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3912896853605058\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10668249 0.13335018 0.3085754  0.45139194]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  600 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.39147056774759315\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10666376 0.1329156  0.30886775 0.451553  ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  601 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3916508491965174\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1066467  0.13252494 0.3091309  0.4516974 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  602 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39265972009337224\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10653977 0.1320509  0.30894578 0.4524636 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  604 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39301456399389\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10643028 0.13159075 0.30870223 0.45327675]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  605 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3931911076176625\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10633138 0.13117704 0.30848202 0.4540096 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  606 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3933670695490996\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10624208 0.13080494 0.30828297 0.45467   ]] action proba 0.07692307692307693 length 13 rew 1\n",
      "==========================================\n",
      "Episode:  608 length 13\n",
      "Reward:  0.07692307692307693\n",
      "Mean Reward val 0.3922015341432291\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10614897 0.13046251 0.30822754 0.455161  ]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  609 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.391763498841355\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10617486 0.13016973 0.308068   0.45558736]] action proba 0.0 length 12 rew 0\n",
      "==========================================\n",
      "Episode:  611 length 12\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.3904832259693244\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10619812 0.12990631 0.30792403 0.4559716 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  612 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.391477543708363\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10617407 0.12960626 0.3085294  0.4556902 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  613 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3916542903798478\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10617585 0.12965477 0.30863208 0.4555373 ]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  615 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.390653248311515\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10616206 0.12967165 0.3086403  0.455526  ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  616 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3908304715719501\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10614961 0.12968688 0.30864766 0.4555158 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  617 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3907374341314345\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10611602 0.12953697 0.3085892  0.45575777]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  619 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39108989402133315\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.10606693 0.12936243 0.30868277 0.45588785]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  621 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.39063622876724524\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10600513 0.12917358 0.30871546 0.4561059 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  622 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3908117725412946\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10594942 0.12900354 0.30874482 0.45630226]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  623 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3909867536750426\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.1056143  0.12881766 0.30920506 0.456363  ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  624 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3911611748691625\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10531302 0.12865016 0.3096199  0.45641696]] action proba 0.05 length 20 rew 1\n",
      "==========================================\n",
      "Episode:  625 length 20\n",
      "Reward:  0.05\n",
      "Mean Reward val 0.3906161889668156\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10503234 0.12848844 0.31014186 0.45633733]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  626 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39158809297165326\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10468861 0.12822272 0.31018046 0.4569082 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  627 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39255690174080665\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10433653 0.12792243 0.31095296 0.4567881 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  628 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3927277174773078\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10401989 0.12765199 0.31164977 0.45667836]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  629 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.39263344067707917\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10358089 0.12736586 0.31192952 0.4571238 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  630 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3935959867298889\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10309785 0.12699036 0.3117479  0.45816386]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  631 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3945554867508859\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10266957 0.12721933 0.31143555 0.4586756 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  632 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.39472206576075813\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10228485 0.1274257  0.31115347 0.45913598]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  633 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.39462523810708705\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10181962 0.12760942 0.31137955 0.4591914 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  634 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3955785841888082\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10131512 0.12765683 0.3111504  0.4598777 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  635 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.395480714297526\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.10076614 0.12799115 0.31100553 0.4602372 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  636 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3964297241651908\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.10023387 0.12823252 0.31161213 0.45992148]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  637 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3962002104909507\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09979561 0.1287455  0.3122819  0.45917705]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  638 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.395775797016004\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09942316 0.12939754 0.31285465 0.4583247 ]] action proba 0.0 length 19 rew 0\n",
      "==========================================\n",
      "Episode:  639 length 19\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.39515739733316646\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09908806 0.12998721 0.31336972 0.4575551 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  640 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3961009895370149\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09874663 0.13045622 0.31456777 0.4562293 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  645 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39458318002047454\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09843061 0.13086385 0.31560794 0.4550976 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  646 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3955189092631013\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09810597 0.13116561 0.31727678 0.4534517 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  647 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39645175045251013\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09777284 0.1313698  0.31950942 0.451348  ]] action proba 0.08333333333333333 length 12 rew 1\n",
      "==========================================\n",
      "Episode:  651 length 12\n",
      "Reward:  0.08333333333333333\n",
      "Mean Reward val 0.39414734298552134\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09747652 0.13161896 0.32151362 0.44939095]] action proba 0.1 length 10 rew 1\n",
      "==========================================\n",
      "Episode:  653 length 10\n",
      "Reward:  0.1\n",
      "Mean Reward val 0.3930949046277674\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09719362 0.1318308  0.32345557 0.44752   ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  654 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39402147729245784\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.0968961  0.13194944 0.3259337  0.44522074]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  657 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3937447836269907\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09661435 0.13203561 0.32811832 0.44323167]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  658 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.3934002037833484\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09636324 0.13207224 0.3304251  0.44113946]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  659 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39431929438367663\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09605885 0.1319829  0.33204544 0.43991274]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  660 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3952356040744729\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.0957409  0.13182689 0.33423132 0.43820092]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  661 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3953938584489827\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09545343 0.13168392 0.3362071  0.43665558]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  663 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.39495592514040145\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09517723 0.13152605 0.33785868 0.43543807]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  664 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.395865765854476\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09488349 0.13130638 0.34007156 0.43373856]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  665 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.39602212356340316\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09463125 0.1311249  0.3426656  0.43157822]] action proba 0.0 length 14 rew 0\n",
      "==========================================\n",
      "Episode:  666 length 14\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.39542838724621665\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09440196 0.13095805 0.345011   0.42962903]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  667 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39633343457069836\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09412274 0.13069165 0.34664437 0.42854118]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  668 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.39595454624227755\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.0940286  0.13044243 0.34812394 0.427405  ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  669 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39685610662102044\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09387326 0.13010572 0.3489707  0.42705035]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  670 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39775497978551966\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09366404 0.12969226 0.34924668 0.42739704]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  671 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.397411098367188\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09347089 0.12931386 0.34922394 0.4279913 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  672 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39830647563558746\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09322836 0.12886436 0.348717   0.42919028]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  674 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3986077897818524\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09298763 0.12845702 0.34832296 0.43023232]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  675 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.398511229935035\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09264595 0.12807064 0.34817857 0.4311049 ]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  676 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.3981687712005175\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09230281 0.12772144 0.34829524 0.43168044]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  677 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3983189647533191\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09227064 0.12746538 0.34855837 0.4317055 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  678 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3992050929348312\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09217402 0.12712726 0.3483078  0.432391  ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  679 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4000886148569858\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09204192 0.1267469  0.34880713 0.43240407]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  681 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3992819033764668\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09192514 0.1265245  0.34921786 0.4323325 ]] action proba 0.1111111111111111 length 9 rew 1\n",
      "==========================================\n",
      "Episode:  682 length 9\n",
      "Reward:  0.1111111111111111\n",
      "Mean Reward val 0.39885998420770347\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09198875 0.12635559 0.34949678 0.43215892]] action proba 0.0 length 8 rew 0\n",
      "==========================================\n",
      "Episode:  683 length 8\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.3982768555758208\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09217278 0.12586908 0.35020605 0.4317521 ]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  685 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.3972979143059205\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.0920838  0.12605324 0.35011083 0.4317521 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  686 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.397204807201157\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09224731 0.12594487 0.3505128  0.43129498]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  687 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.39683511582856384\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09239732 0.12585054 0.35101762 0.43073452]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  689 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.39616796090345696\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09252302 0.12574823 0.3516178  0.430111  ]] action proba 0.08333333333333333 length 12 rew 1\n",
      "==========================================\n",
      "Episode:  691 length 12\n",
      "Reward:  0.08333333333333333\n",
      "Mean Reward val 0.39514339068889975\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09271941 0.12565227 0.35209775 0.4295306 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  692 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3952946989274439\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09293319 0.1259035  0.3526691  0.42849424]] action proba 0.1111111111111111 length 9 rew 1\n",
      "==========================================\n",
      "Episode:  693 length 9\n",
      "Reward:  0.1111111111111111\n",
      "Mean Reward val 0.39488521248966824\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.093274   0.12613213 0.35316265 0.42743132]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  695 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.39393008256872086\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09356397 0.12631956 0.35353288 0.42658356]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  696 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.39365184715614016\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.0938477  0.12669164 0.35394856 0.42551208]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  697 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.39344604221752105\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09410099 0.12702365 0.35414416 0.42473122]] action proba 0.0 length 8 rew 0\n",
      "==========================================\n",
      "Episode:  698 length 8\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.392883172343104\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09432955 0.12732324 0.35431954 0.42402765]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  699 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39375048209689956\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09448495 0.12751165 0.35520577 0.42279756]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  700 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.3935454172151636\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09463228 0.12769058 0.35653538 0.42114174]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  703 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39328883163043993\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.0947661  0.12793574 0.35771263 0.41958556]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  704 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.39320378837044406\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09488222 0.12815075 0.35848227 0.41848472]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  705 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.3928491897224082\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09498375 0.12827136 0.35957512 0.4171698 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  706 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.39276500887885923\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09506164 0.12822199 0.36078802 0.41592845]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  707 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3936226854199908\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09506281 0.12807126 0.3613705  0.4154954 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  708 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3937727239454915\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09527194 0.12627405 0.3680753  0.41037866]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  709 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.39368759804322095\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09538221 0.12849335 0.36129734 0.41482714]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  710 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39454035810223187\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09557562 0.12859038 0.36068606 0.415148  ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  711 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3946884755768074\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09575007 0.1286777  0.36013556 0.4154367 ]] action proba 0.07692307692307693 length 13 rew 1\n",
      "==========================================\n",
      "Episode:  712 length 13\n",
      "Reward:  0.07692307692307693\n",
      "Mean Reward val 0.3942428018059045\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09592115 0.12890644 0.35953477 0.41563767]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  715 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39398759453576804\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09606401 0.12909149 0.35892963 0.41591483]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  716 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.393786775017587\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09619499 0.12926129 0.35856265 0.415981  ]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  718 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.392923204943361\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09630573 0.12941012 0.35838664 0.41589743]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  719 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.3925758909682413\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09636591 0.12949498 0.3580512  0.41608796]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  720 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.39220477322764735\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09639233 0.12953736 0.3579556  0.41611463]] action proba 0.06666666666666667 length 15 rew 1\n",
      "==========================================\n",
      "Episode:  721 length 15\n",
      "Reward:  0.06666666666666667\n",
      "Mean Reward val 0.3917538894235463\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.0963609  0.12953949 0.35774523 0.41635442]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  722 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.39167308644140214\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09634937 0.12970579 0.35761815 0.41632676]] action proba 0.07142857142857142 length 14 rew 1\n",
      "==========================================\n",
      "Episode:  723 length 14\n",
      "Reward:  0.07142857142857142\n",
      "Mean Reward val 0.3912307597632076\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09634428 0.13003114 0.3574435  0.41618103]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  724 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39207044147387904\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09626882 0.13021548 0.3567817  0.41673395]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  725 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39290781001179376\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09614597 0.1302936  0.3569166  0.4166439 ]] action proba 0.1111111111111111 length 9 rew 1\n",
      "==========================================\n",
      "Episode:  728 length 9\n",
      "Reward:  0.1111111111111111\n",
      "Mean Reward val 0.3914433212341199\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09602606 0.13034168 0.35714132 0.41649088]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  729 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3922769605201006\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09642027 0.13038567 0.35714322 0.4160509 ]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  732 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.3909443126598546\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09676508 0.13041048 0.35723615 0.4155883 ]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  733 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.3906387572838421\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09708013 0.13043728 0.35759494 0.41488764]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  735 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.38974707587817947\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09745275 0.13045771 0.3578734  0.41421616]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  736 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39057509884170977\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09773345 0.13038631 0.3588504  0.41302985]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  738 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.39019465202481746\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.0979812  0.13030867 0.35988173 0.41182837]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  739 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.3898362808734326\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09817223 0.13019788 0.36066815 0.41096172]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  740 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3906597136927667\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09827302 0.12998924 0.3608674  0.4108703 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  741 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.390582454420045\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09825952 0.130107   0.3610984  0.41053504]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  742 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.39072971894976233\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09825364 0.13022153 0.3619225  0.4096023 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  743 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.39087658760708793\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09825388 0.13033217 0.36327907 0.40813485]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  744 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3910230619861388\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.0982869  0.13077892 0.3646275  0.40630668]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  745 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.39116914367248445\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.0983154  0.13118114 0.3658422  0.40466127]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  747 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3907916860690821\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09833657 0.1315142  0.3668036  0.40334567]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  748 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.3904924537334314\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09830481 0.13170557 0.36807156 0.40191802]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  750 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.3897188386768843\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09826718 0.1318314  0.36933798 0.40056345]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  754 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.38798522893554976\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09823602 0.1320206  0.3704541  0.39928925]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  756 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.38722436967812435\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09820161 0.13218322 0.37136433 0.39825085]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  757 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.38803278079992104\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09816816 0.13292062 0.37190935 0.39700186]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  758 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3881803001928065\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.0981048  0.1332345  0.37227502 0.39638573]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  759 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3889853261136054\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09804609 0.1341005  0.37233016 0.39552328]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  760 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3897882363289621\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09792516 0.13477863 0.3718542  0.395442  ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  761 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39058903916842536\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09774895 0.13528559 0.37090507 0.39606047]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  762 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3905139989248668\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09759099 0.13574475 0.3703419  0.39632234]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  763 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3913117554707767\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09738192 0.1360516  0.3693188  0.39724764]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  765 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3915955367880854\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09716723 0.1363083  0.36834106 0.39818347]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  766 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3923887629461192\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09691247 0.13643777 0.36818042 0.39846924]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  767 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3931799234110331\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09661705 0.13644516 0.36752427 0.39941356]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  768 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39396902624144786\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09628522 0.13634205 0.36642465 0.40094802]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  769 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.39410672880477066\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09627447 0.13630092 0.36557537 0.4018492 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  770 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3948925825936101\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09620515 0.1361632  0.36552873 0.40210286]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  771 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.3948128426334285\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09612636 0.13587086 0.3651429  0.4028599 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  774 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39457485743613774\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09604387 0.13558117 0.36473027 0.4036447 ]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  775 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.39422746715593654\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.0959386  0.1352811  0.3642178  0.40456256]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  777 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3938567024588776\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09583647 0.13498351 0.36391324 0.40526673]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  778 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3946348068202911\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09567805 0.13460688 0.3631346  0.4065805 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  779 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3954109160423164\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09546871 0.1341592  0.36193138 0.40844074]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  780 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39618503778874103\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09521309 0.13364738 0.36034882 0.41079068]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  781 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3969571796841519\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09497976 0.13377427 0.35867196 0.4125739 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  782 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3977273493141849\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09471316 0.13379432 0.3578855  0.41360697]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  784 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39798791657707866\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.0944676  0.13380495 0.35728744 0.41444   ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  785 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.3981177029427567\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09397153 0.13376865 0.35721007 0.41504976]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  786 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.39803538481110556\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09353274 0.1337445  0.35771096 0.41501176]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  787 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.3987992992973859\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.0930737  0.13361314 0.35766798 0.4156452 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  788 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39956127737178715\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.09259713 0.1333856  0.3571361  0.41688117]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  790 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.39981523115845774\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.09215908 0.1331407  0.35656178 0.41813853]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  791 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4005730402100254\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09170161 0.13281085 0.35555327 0.41993433]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  792 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40132893801556124\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.09122662 0.1324043  0.35415632 0.42221278]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  793 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4020829317963981\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.0907502  0.13194937 0.3536175  0.4236829 ]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  794 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.4018916325111196\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09032926 0.13154985 0.35365254 0.42446843]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  795 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.40201488422907045\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08995128 0.13119039 0.35368267 0.42517573]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  796 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40276517923003774\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.0895646  0.13078186 0.3544262  0.42522725]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  797 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4035135937924061\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08915509 0.13030735 0.3546083  0.4259293 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  800 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.40262652664961307\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08878158 0.1298689  0.35487947 0.42647004]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  801 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.4024362192597757\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08844831 0.12947795 0.35529736 0.42677638]] action proba 0.0625 length 16 rew 1\n",
      "==========================================\n",
      "Episode:  802 length 16\n",
      "Reward:  0.0625\n",
      "Mean Reward val 0.40201288648361155\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08813661 0.12910038 0.35554272 0.42722034]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  804 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.40132465570973924\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.087846   0.1287266  0.35592344 0.42750397]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  805 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.4009818211493053\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08758196 0.1283863  0.35603622 0.42799553]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  806 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.4011045202556878\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08731958 0.12775412 0.35661075 0.42831555]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  808 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4013490084627195\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08708132 0.12717731 0.35700795 0.42873344]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  809 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.4014707998102964\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08686689 0.12665938 0.35736492 0.42910877]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  810 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.40138678320551596\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08657544 0.12648027 0.35745126 0.42949304]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  811 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40212399160058304\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.0862693  0.12623955 0.35825098 0.42924017]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  812 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4028593864448628\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.0859348  0.1259211  0.3584719  0.42967218]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  813 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40359297442220327\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08559068 0.12555592 0.35939005 0.4294633 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  815 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.4032165210535214\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08527175 0.12521492 0.36037576 0.4291376 ]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  816 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.40296778602163214\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.0849259  0.12483048 0.3612051  0.42903847]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  817 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4036976542538795\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08462254 0.12504601 0.3617247  0.42860678]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  819 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.4030179038776505\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08435591 0.1253573  0.3621234  0.4281633 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  820 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.40293302620341875\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08435697 0.12568077 0.36205393 0.42790833]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  821 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40365938505231974\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08431454 0.12589331 0.36271006 0.42708218]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  823 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.40288189463552604\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08426374 0.12606426 0.3634361  0.42623588]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  825 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.40251172055650536\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.0842083  0.12621163 0.3642274  0.42535257]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  826 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40323419731520366\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.08461626 0.1263554  0.3647525  0.4242758 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  827 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40395492896095825\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.08494109 0.12640256 0.3659392  0.42271718]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  828 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.404673921809015\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.08518953 0.12636164 0.3677188  0.42073008]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  829 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.40478877250563067\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08541771 0.12633024 0.36993062 0.41832143]] action proba 0.09090909090909091 length 11 rew 1\n",
      "==========================================\n",
      "Episode:  832 length 11\n",
      "Reward:  0.09090909090909091\n",
      "Mean Reward val 0.4034400843584425\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08561724 0.12628983 0.37200466 0.41608822]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  834 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.402673361601496\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08579743 0.1263604  0.3738214  0.41402072]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  835 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4033878671498196\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.08591108 0.1263362  0.37616262 0.4115901 ]] action proba 0.08333333333333333 length 12 rew 1\n",
      "==========================================\n",
      "Episode:  836 length 12\n",
      "Reward:  0.08333333333333333\n",
      "Mean Reward val 0.4030054841942444\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08599792 0.12629358 0.37847662 0.4092319 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  837 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.40312122943983597\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08581819 0.12620467 0.38102126 0.4069558 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  838 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.403038049587504\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08565274 0.12611896 0.38273194 0.40549642]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  840 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4032686368655361\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08549648 0.12603164 0.38412425 0.4043476 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  841 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.40338351971961506\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08535531 0.12595211 0.3853802  0.40331236]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  843 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4036124687250188\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08522778 0.12600367 0.38641137 0.4023572 ]] action proba 0.0625 length 16 rew 1\n",
      "==========================================\n",
      "Episode:  845 length 16\n",
      "Reward:  0.0625\n",
      "Mean Reward val 0.4027321792008462\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08510746 0.12604263 0.38746393 0.40138596]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  846 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40343733601406834\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.0849489  0.12598668 0.38911045 0.39995396]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  848 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40366480989860526\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08479332 0.12591876 0.39070266 0.39858532]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  849 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40436638071048925\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.0846023  0.12576507 0.39159054 0.39804208]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  850 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.4044787586414993\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08443079 0.12562707 0.39176813 0.39817405]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  851 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40517772723464307\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.08422644 0.1254116  0.39262274 0.39773926]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  852 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4058750569799717\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.08399324 0.12512743 0.39284647 0.39803284]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  853 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.40598527354088504\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08378357 0.12487182 0.3930476  0.398297  ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  854 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40668002760691907\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.08354551 0.12455147 0.39392167 0.39798135]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  855 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.40643857897653723\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08333102 0.12426253 0.39520523 0.39720118]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  857 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40665667086703483\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08312483 0.12397237 0.39623904 0.39666373]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  858 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4073474081535691\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08289154 0.12362369 0.3966187  0.39686602]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  861 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40708981856602766\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08267235 0.12329083 0.3970684  0.3969684 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  862 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.407776852379972\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.08242606 0.12290204 0.3981635  0.39650843]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  863 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4084622958378656\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.08215542 0.12246313 0.3998377  0.39554372]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  864 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.4083754415459529\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08191232 0.12206902 0.40105364 0.39496496]] action proba 0.0 length 8 rew 0\n",
      "==========================================\n",
      "Episode:  865 length 8\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.40790387637095754\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.08169349 0.12171437 0.4021501  0.39444208]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  866 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40858680154238664\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.08145106 0.12131187 0.40257707 0.39466   ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  867 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40926815315351295\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.08118428 0.12086183 0.40364674 0.39430717]] action proba 0.1111111111111111 length 9 rew 1\n",
      "==========================================\n",
      "Episode:  868 length 9\n",
      "Reward:  0.1111111111111111\n",
      "Mean Reward val 0.40892504953781394\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08103975 0.12045126 0.40456474 0.39394417]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  869 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.40883816250769384\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08091062 0.12008341 0.4050981  0.3939079 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  870 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.4089428259261695\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08079431 0.11975271 0.40557852 0.3938745 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  872 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4091514334269114\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08068355 0.11944143 0.40589514 0.39397988]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  873 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40982746153511856\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.08053509 0.11907455 0.40686598 0.39352444]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  874 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4105019444362213\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.08035264 0.11865794 0.40842372 0.3925657 ]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  876 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.40972868702913434\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08018221 0.11825501 0.40976888 0.3917939 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  877 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.40964167637572224\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08002993 0.11789454 0.4106848  0.3913907 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  878 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4103133013172743\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07984391 0.11748418 0.41219077 0.39048117]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  881 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.4094845712674423\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07967584 0.11710833 0.41347086 0.38974488]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  882 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.4095870802467544\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07952801 0.11677556 0.41399318 0.38970318]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  883 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.40968935730529876\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07942113 0.11678801 0.4146004  0.38919058]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  884 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.40979140322924756\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07929492 0.11648433 0.41563863 0.38858214]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  887 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4095330989390587\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07918455 0.11628962 0.41651985 0.38800594]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  890 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.40871536684386545\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07908102 0.11610286 0.4172174  0.3875987 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  891 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40937824199314365\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07894675 0.11586138 0.41726077 0.3879311 ]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  892 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.40907978611505186\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07882735 0.11564629 0.41707847 0.38844794]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  893 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4097407706943415\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.0786709  0.11536793 0.417592   0.38836914]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  896 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.4089278138246837\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07852619 0.11510015 0.4181637  0.38821003]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  899 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4086758322230459\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07839155 0.11484522 0.41858193 0.3881813 ]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  900 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.40859221124758555\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07825417 0.11446641 0.41948858 0.38779086]] action proba 0.0 length 10 rew 0\n",
      "==========================================\n",
      "Episode:  901 length 10\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.4081392265344508\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07813034 0.11412583 0.4203057  0.38743812]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  903 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.4073942914567829\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07800752 0.11380113 0.4209068  0.38728452]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  904 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.4073124561439393\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.08091161 0.10397572 0.437995   0.37711763]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  905 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.40723080148300045\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07761319 0.11360799 0.42226642 0.38651243]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  906 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.40733308284850983\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.0776322  0.11242765 0.42403653 0.38590363]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  907 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.4074351389246678\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07720437 0.11245545 0.42315385 0.38718632]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  909 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.4070891276303279\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07759009 0.10987025 0.42695105 0.38558865]] action proba 0.1111111111111111 length 9 rew 1\n",
      "==========================================\n",
      "Episode:  910 length 9\n",
      "Reward:  0.1111111111111111\n",
      "Mean Reward val 0.40676423408859436\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07685313 0.1114136  0.42389357 0.38783962]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  913 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.4059761676747369\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07670412 0.11104086 0.4242625  0.38799253]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  915 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.4053626825924776\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07656172 0.11070001 0.42451912 0.38821918]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  916 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4060111420443942\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.0764489  0.11091424 0.42444366 0.38819322]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  918 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4062156879811855\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07632978 0.11109056 0.4245088  0.38807082]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  919 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.4060458883203364\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07621056 0.11115888 0.4250803  0.38755026]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  920 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4066907896359495\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07606526 0.11115491 0.4249902  0.38778958]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  921 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.4066112262343198\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07593693 0.11115475 0.42460653 0.38830182]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  923 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.4058663967403061\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07580867 0.11114907 0.4241524  0.38888985]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  924 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4065087033384247\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07570889 0.11165875 0.42344165 0.38919064]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  925 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.40642968026066545\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07583292 0.1120268  0.4225874  0.38955292]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  926 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4070699934426928\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07589718 0.11227884 0.42250004 0.38932392]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  927 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40770892663941405\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07591657 0.11243909 0.42182225 0.38982207]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  928 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.4076288667973192\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07593186 0.11258046 0.42151335 0.38997433]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  930 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.40702171563341516\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07593956 0.11269546 0.4213715  0.38999343]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  933 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4067850291806312\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.0790676  0.10229127 0.44114226 0.37749878]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  934 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.4064836548178711\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07595186 0.11301874 0.42084157 0.39018783]] action proba 0.09090909090909091 length 11 rew 1\n",
      "==========================================\n",
      "Episode:  935 length 11\n",
      "Reward:  0.09090909090909091\n",
      "Mean Reward val 0.40614650250600276\n",
      "Max reward so far:  1.0\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tooooooooo long\n",
      "3596\n",
      "[[0.07594159 0.11314151 0.42065677 0.39026016]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  937 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.4058135675326424\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07592013 0.11323111 0.42064017 0.39020854]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  938 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40644635393569606\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07586192 0.11324309 0.4200278  0.3908672 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  939 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4070777939847006\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07576233 0.11317264 0.42015997 0.39090508]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  940 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.40717654234390926\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.075642   0.11280826 0.42076913 0.39078057]] action proba 0.0 length 19 rew 0\n",
      "==========================================\n",
      "Episode:  941 length 19\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.4067442954836715\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07553364 0.11248079 0.4213179  0.3906677 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  942 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40737341075887445\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07539754 0.11211903 0.42121354 0.39126992]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  943 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4080011931627316\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.0752366  0.11172686 0.42052245 0.3925141 ]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  944 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.40774581271141297\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07505707 0.11136908 0.4196828  0.39389113]] action proba 0.16666666666666666 length 6 rew 1\n",
      "==========================================\n",
      "Episode:  947 length 6\n",
      "Reward:  0.16666666666666666\n",
      "Mean Reward val 0.406631286581173\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07489018 0.11103734 0.41882542 0.3952471 ]] action proba 0.0 length 8 rew 0\n",
      "==========================================\n",
      "Episode:  949 length 8\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.4057752207146863\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07473964 0.11073859 0.41805175 0.3964701 ]] action proba 0.125 length 8 rew 1\n",
      "==========================================\n",
      "Episode:  950 length 8\n",
      "Reward:  0.125\n",
      "Mean Reward val 0.40547997863191587\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07460472 0.11047079 0.41717193 0.39775258]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  952 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40567834174076806\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07446718 0.11021504 0.4162106  0.39910713]] action proba 0.0 length 16 rew 0\n",
      "==========================================\n",
      "Episode:  953 length 16\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.40525310238883855\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07434303 0.10998442 0.41534376 0.40032881]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  954 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.4051777937301416\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.0742024  0.10950031 0.41502118 0.4012761 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  957 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.404952811077542\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07430842 0.1081996  0.41613123 0.40136078]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  959 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.4046299927211304\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07393769 0.10862697 0.4141931  0.40324223]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  961 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.40404864138491187\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07379363 0.10823011 0.4136542  0.404322  ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  962 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40466749014775205\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07362089 0.10780198 0.41386405 0.40471303]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  963 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40528505499199713\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07342286 0.10734677 0.41474634 0.40448397]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  964 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4059013399091039\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07320539 0.1068723  0.41494557 0.4049767 ]] action proba 0.0 length 11 rew 0\n",
      "==========================================\n",
      "Episode:  965 length 11\n",
      "Reward:  0.0\n",
      "Mean Reward val 0.40548115218663067\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07300983 0.10644634 0.4151239  0.4054199 ]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  966 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.4052095658274482\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07285418 0.10626366 0.41524646 0.40563568]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  967 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4058240187553124\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07267529 0.10603447 0.41476077 0.4065295 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  968 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40643720346247925\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07247543 0.10576346 0.41372946 0.40803173]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  969 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.4063618386479131\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07228251 0.10538705 0.41242033 0.40991008]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  971 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4065545097618063\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07210561 0.10502347 0.41140422 0.41146663]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  972 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40716442290696375\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07190567 0.10462974 0.4111872  0.41227734]] action proba 0.25 length 4 rew 1\n",
      "==========================================\n",
      "Episode:  973 length 4\n",
      "Reward:  0.25\n",
      "Mean Reward val 0.40700306312985185\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07172592 0.10427614 0.41080442 0.4131935 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  974 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40761126511638535\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07152444 0.10389313 0.4111576  0.4134249 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  975 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40821822078737263\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07130381 0.10348437 0.41217166 0.41304016]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  976 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.40831216324306624\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07110531 0.10311706 0.41308558 0.41269207]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  977 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.4082354977728109\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07092702 0.10278737 0.41360438 0.4126812 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  979 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.4079125681855194\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07076021 0.10247347 0.41421187 0.41255444]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  980 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40851612316188485\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07103029 0.10221981 0.41452992 0.41221994]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  981 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.40911844890204585\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07123502 0.10192756 0.41551062 0.41132668]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  982 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4097195491574863\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07138005 0.1016006  0.41708568 0.40993375]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  983 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4103194276644401\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07147267 0.10124541 0.4179046  0.40937737]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  984 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4109180881439686\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07151584 0.10086247 0.41933042 0.40829125]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  985 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.41151553430203763\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "tooooooooo long\n",
      "3596\n",
      "[[0.07151379 0.1004544  0.42130056 0.4067313 ]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  987 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.411188579779159\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07150159 0.10008011 0.42296413 0.40545416]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  988 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4117839401636088\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07151004 0.10022576 0.42419195 0.40407217]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  989 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.41237809779980716\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.0714808  0.10029954 0.42468962 0.40353   ]] action proba 0.2 length 5 rew 1\n",
      "==========================================\n",
      "Episode:  990 length 5\n",
      "Reward:  0.2\n",
      "Mean Reward val 0.41216379094027145\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07144982 0.10031374 0.4248465  0.40338996]] action proba 0.14285714285714285 length 7 rew 1\n",
      "==========================================\n",
      "Episode:  991 length 7\n",
      "Reward:  0.14285714285714285\n",
      "Mean Reward val 0.41189231246438124\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07142095 0.10032512 0.4252138  0.40304008]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  992 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.41198104125344026\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07139492 0.10033531 0.4255448  0.40272495]] action proba 0.3333333333333333 length 3 rew 1\n",
      "==========================================\n",
      "Episode:  993 length 3\n",
      "Reward:  0.3333333333333333\n",
      "Mean Reward val 0.4119019188108647\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07137144 0.10034447 0.425843   0.40244108]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  994 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4124929721587935\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07176701 0.10038168 0.42586523 0.4019861 ]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  995 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.4130828386526099\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07208314 0.10034987 0.42656496 0.40100205]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  996 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.41317001735004966\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07237191 0.10032504 0.4265493  0.40075374]] action proba 1.0 length 1 rew 1\n",
      "==========================================\n",
      "Episode:  997 length 1\n",
      "Reward:  1.0\n",
      "Mean Reward val 0.41375802334468886\n",
      "Max reward so far:  1.0\n",
      "ATTTTTTTTTTTTTTTTTT\n",
      "3596\n",
      "[[0.07259668 0.10024608 0.4259273  0.40122986]] action proba 0.08333333333333333 length 12 rew 1\n",
      "==========================================\n",
      "Episode:  998 length 12\n",
      "Reward:  0.08333333333333333\n",
      "Mean Reward val 0.4134272678992321\n",
      "Max reward so far:  1.0\n",
      "3596\n",
      "[[0.07279895 0.1001623  0.4251973  0.40184143]] action proba 0.5 length 2 rew 1\n",
      "==========================================\n",
      "Episode:  999 length 2\n",
      "Reward:  0.5\n",
      "Mean Reward val 0.4135138406313329\n",
      "Max reward so far:  1.0\n",
      "Model saved\n",
      "0.708\n"
     ]
    }
   ],
   "source": [
    "# Parameters for the training\n",
    "max_episodes = 1000\n",
    "gamma = 0.95 # Discount rate\n",
    "#max_batch = NbStat*5\n",
    "max_batch = 20\n",
    "episodes_succeded = 0\n",
    "\n",
    "client.run() # This run the main loop of ROS\n",
    "teresa_controller = Teresa(client) # Robot API\n",
    "env = RobotEnv(teresa_controller, client) # Training Environment\n",
    "\n",
    "env.reset() # Restarting the environment to the initial state\n",
    "write_op = tf.summary.merge_all()\n",
    "allRewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "episode = 0\n",
    "episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "NbStat = state_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #writer = tf.summary.FileWriter(\"./tensorboard/pg/1\",sess.graph)\n",
    "     \n",
    "    for episode in range(max_episodes):\n",
    "        \n",
    "        episode_rewards_sum = 0\n",
    "\n",
    "        # Launch the game\n",
    "        state = env.reset()\n",
    "        print(NbStat)\n",
    "        ne_state=np.identity(NbStat)[state:state+1]\n",
    "        episode_length=0\n",
    "        while True:\n",
    "            episode_length+=1\n",
    "            if episode_length > max_batch:\n",
    "                print (\"tooooooooo long\")\n",
    "                break      \n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            #state=int(state)\n",
    "            #print(\"state\",state,NbStat)\n",
    "            ne_state=np.identity(NbStat)[state:state+1]\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: ne_state.reshape([1,NbStat])})\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "\n",
    "            # Perform a\n",
    "            ext=False\n",
    "            #print(\"in actor mstep\",state,\"real ibn self\")\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            env.render()\n",
    "            #print(\"after action state\",new_state,NbStat)\n",
    "            #print(\" drz actor mstep\",state,\"real ibn self\",new_state,\"rew\",reward,\"done\",done)\n",
    "            # Store s, a, r\n",
    "            episode_states.append(ne_state)\n",
    "                        \n",
    "            # For actions because we output only one (the index) we need 2 (1 is for the action taken)\n",
    "            # We need [0., 1.] (if we take right) not just the index\n",
    "            action_ = np.zeros((action_size), dtype=int)\n",
    "            action_[action] = 1\n",
    "            \n",
    "            #print(\"action proba\",action_probability_distribution,\"st\",state,\"new\",new_state)\n",
    "            state = new_state\n",
    "            episode_actions.append(action_)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            if done:\n",
    "                # Calculate sum reward\n",
    "                if reward == 1:\n",
    "                    episodes_succeded += 1\n",
    "                \n",
    "                episode_rewards_sum = np.sum(episode_rewards)/episode_length  # HA addded the sum \n",
    "                print(action_probability_distribution,\"action proba\",episode_rewards_sum,\"length\",episode_length,\"rew\",reward)\n",
    "                allRewards.append(episode_rewards_sum)\n",
    "                \n",
    "                total_rewards = np.sum(allRewards)\n",
    "                \n",
    "                # Mean reward\n",
    "                mean_reward = np.divide(total_rewards, episode+1)\n",
    "                \n",
    "                \n",
    "                maximumRewardRecorded = np.amax(allRewards)\n",
    "                \n",
    "                print(\"==========================================\")\n",
    "                print(\"Episode: \", episode,\"length\",episode_length)\n",
    "                print(\"Reward: \", episode_rewards_sum)\n",
    "                print(\"Mean Reward\",\"val\",mean_reward)\n",
    "                print(\"Max reward so far: \", maximumRewardRecorded)\n",
    "                \n",
    "                # Calculate discounted reward\n",
    "                discounted_episode_rewards = discount_and_normalize_rewards(episode_rewards)\n",
    "                \n",
    "                #print(\"disco\",discounted_episode_rewards)               \n",
    "                # Feedforward, gradient and backpropagation\n",
    "                #loss_, _ = sess.run([loss, train_opt], feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                #                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                #                                                 discounted_episode_rewards_: discounted_episode_rewards \n",
    "                #                                                })\n",
    "                loss_, _ = sess.run([loss, train_opt], feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                                                                 discounted_episode_rewards_: discounted_episode_rewards\n",
    "                                                                })\n",
    "                \n",
    " \n",
    "                                                                 \n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={input_: np.vstack(np.array(episode_states)),actions: np.vstack(np.array(episode_actions)),discounted_episode_rewards_: discounted_episode_rewards,\n",
    "                                                                    mean_reward_: mean_reward  })\n",
    "                \n",
    "               \n",
    "#                 writer.add_summary(summary, episode)\n",
    "#                 writer.flush()\n",
    "                \n",
    "            \n",
    "                \n",
    "                # Reset the transition stores\n",
    "                episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "                \n",
    "                break\n",
    "            \n",
    "            \n",
    "            \n",
    "        # Save Model\n",
    "    saver.save(sess, \"pgpendul.ckpt\")\n",
    "    print(\"Model saved\")\n",
    "    print(episodes_succeded / max_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708\n"
     ]
    }
   ],
   "source": [
    "print(episodes_succeded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "NbStat = NB_STATES\n",
    "state_size = NbStat\n",
    "action_size = 4\n",
    "# new_graph = tf.Graph()\n",
    "initializer=tf.initializers.glorot_uniform()\n",
    "learning_rate = 0.01\n",
    "\n",
    "def discount_correct_rewards(r, gamma=0.99):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(range(0, r.size)):\n",
    "    #if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "\n",
    "  discounted_r -= discounted_r.mean()\n",
    "  discounted_r /- discounted_r.std()\n",
    "  return discounted_r\n",
    "\n",
    "# Setup TensorBoard Writer\n",
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    #print(\"len episode rewards\",episode_rewards)\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        #print(\"dans boucle\",episode_rewards[i],\"cyl\",cumulative)\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    if std :\n",
    "        discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    else:\n",
    "        discounted_episode=[]\n",
    "        discounted_episode_rewards[0] = np.array(mean)\n",
    "        print(\"ATTTTTTTTTTTTTTTTTT\")\n",
    "    #print(\"dis\",discounted_episode_rewards,\"std\",std)\n",
    "    \n",
    "    return discounted_episode_rewards\n",
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    #print(\"len episode rewards\",episode_rewards)\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        #print(\"dans boucle\",episode_rewards[i],\"cyl\",cumulative)\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    if std :\n",
    "        discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    else:\n",
    "        discounted_episode=[]\n",
    "        discounted_episode_rewards[0] = np.array(mean)\n",
    "        print(\"ATTTTTTTTTTTTTTTTTT\")\n",
    "    #print(\"dis\",discounted_episode_rewards,\"std\",std)\n",
    "    \n",
    "    return discounted_episode_rewards\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    input_ = tf.placeholder(tf.float32, [None, state_size], name=\"input_\")\n",
    "    actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "    discounted_episode_rewards_ = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards\")\n",
    "    \n",
    "    # Add this placeholder for having this variable in tensorboard\n",
    "    mean_reward_ = tf.placeholder(tf.float32 , name=\"mean_reward\")\n",
    "\n",
    "    with tf.name_scope(\"fc1\"):\n",
    "        fc1 = tf.layers.dense(input_ , 20, activation=tf.nn.relu,kernel_initializer=initializer)\n",
    "\n",
    "    with tf.name_scope(\"fc2\"):\n",
    "        fc2 = tf.layers.dense(fc1, action_size,activation= tf.nn.relu, kernel_initializer=initializer)\n",
    "    \n",
    "    with tf.name_scope(\"fc3\"):\n",
    "        fc3 = tf.layers.dense(fc2, action_size, activation= None,kernel_initializer=initializer)\n",
    "\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        action_distribution = tf.nn.softmax(fc3)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
    "        # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
    "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = fc3, labels = actions)\n",
    "        #loss = tf.nn.sparse_softmax_cross_entropy_with_logits (neg_log_prob * discounted_episode_rewards_)\n",
    "        loss = tf.reduce_mean(neg_log_prob * discounted_episode_rewards_) \n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"test\"):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "## Losses\n",
    "## TRAINING Hyperparameters\n",
    "\n",
    "# tf.summary.scalar(\"Loss\", loss)\n",
    "\n",
    "# ## Reward mean\n",
    "# tf.summary.scalar(\"Reward_mean\", mean_reward_)\n",
    "\n",
    "max_episodes = 500\n",
    "\n",
    "gamma = 0.95 # Discount rate\n",
    "max_batch = NbStat*5\n",
    "    \n",
    "episode_rewards_sum = 0\n",
    "\n",
    "        # Launch the game\n",
    "    #state = env.reset()\n",
    "    #ne_state=np.identity(NbStat)[state:state+1]\n",
    "    #env.render()\n",
    "episode_length=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo\n",
      "INFO:tensorflow:Restoring parameters from pgpendul.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-15 13:56:53,182     INFO: Restoring parameters from pgpendul.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey dense_3/bias not found in checkpoint\n\t [[node save_1/RestoreV2 (defined at /home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'save_1/RestoreV2':\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/asyncio/base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/asyncio/base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 381, in dispatch_queue\n    yield self.process_one()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 250, in wrapper\n    runner = Runner(ctx_run, result, future, yielded)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 741, in __init__\n    self.ctx_run(self.run)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2867, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3072, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-e0cece8504d6>\", line 1, in <module>\n    saver = tf.train.Saver()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 828, in __init__\n    self.build()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 840, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\n    build_restore=build_restore)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 328, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 575, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_io_ops.py\", line 1696, in restore_v2\n    name=name)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key dense_3/bias not found in checkpoint\n\t [[{{node save_1/RestoreV2}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1290\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1291\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key dense_3/bias not found in checkpoint\n\t [[node save_1/RestoreV2 (defined at /home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'save_1/RestoreV2':\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/asyncio/base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/asyncio/base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 381, in dispatch_queue\n    yield self.process_one()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 250, in wrapper\n    runner = Runner(ctx_run, result, future, yielded)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 741, in __init__\n    self.ctx_run(self.run)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2867, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3072, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-e0cece8504d6>\", line 1, in <module>\n    saver = tf.train.Saver()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 828, in __init__\n    self.build()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 840, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\n    build_restore=build_restore)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 328, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 575, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_io_ops.py\", line 1696, in restore_v2\n    name=name)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1299\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m         \u001b[0mnames_to_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_graph_key_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mobject_graph_key_mapping\u001b[0;34m(checkpoint_path)\u001b[0m\n\u001b[1;32m   1617\u001b[0m   \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1618\u001b[0;31m   \u001b[0mobject_graph_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBJECT_GRAPH_PROTO_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1619\u001b[0m   \u001b[0mobject_graph_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrackable_object_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrackableObjectGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mget_tensor\u001b[0;34m(self, tensor_str)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader_GetTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e0cece8504d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pgpendul.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pgpendul.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         \u001b[0;31m# a helpful message (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m         raise _wrap_restore_error_with_msg(\n\u001b[0;32m-> 1306\u001b[0;31m             err, \"a Variable name or other graph key that is missing\")\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m       \u001b[0;31m# This is an object-based checkpoint. We'll print a warning and then do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey dense_3/bias not found in checkpoint\n\t [[node save_1/RestoreV2 (defined at /home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'save_1/RestoreV2':\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/asyncio/base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/asyncio/base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n    ret = callback()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 814, in inner\n    self.ctx_run(self.run)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 381, in dispatch_queue\n    yield self.process_one()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 250, in wrapper\n    runner = Runner(ctx_run, result, future, yielded)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 741, in __init__\n    self.ctx_run(self.run)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 775, in run\n    yielded = self.gen.send(value)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 234, in wrapper\n    yielded = ctx_run(next, result)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tornado/gen.py\", line 162, in _fake_ctx_run\n    return f(*args, **kw)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2867, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3072, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-e0cece8504d6>\", line 1, in <module>\n    saver = tf.train.Saver()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 828, in __init__\n    self.build()\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 840, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 878, in _build\n    build_restore=build_restore)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 508, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 328, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 575, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_io_ops.py\", line 1696, in restore_v2\n    name=name)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/home/saadubuntu/Documents/robot_controller/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "client.run() # This run the main loop of ROS\n",
    "teresa_controller = Teresa(client) # Robot API\n",
    "env = RobotEnv(teresa_controller, client) # Training Environment\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "        # Load the model\n",
    "    print(\"yo\")\n",
    "    print(saver.restore(sess, \"pgpendul.ckpt\"))\n",
    "    if not saver.restore(sess, \"pgpendul.ckpt\"):\n",
    "        print('yo')\n",
    "    total_rewards = 0\n",
    "    for episode in range(50):\n",
    "        state = env.reset()\n",
    "        #ne_state=np.identity(NbStat)[state:state+1]\n",
    "        step = 0\n",
    "        done = False\n",
    "        \n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "\n",
    "       \n",
    "        #while True:\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < 500:\n",
    "            j+=1\n",
    "            state=int(state)\n",
    "            ne_state=np.identity(NbStat)[state:state+1]\n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: ne_state.reshape([1,NbStat])})\n",
    "            print(action_probability_distribution)\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "            #action = np.argmax(action_probability_distribution)\n",
    "            \n",
    "\n",
    "            # new_state, reward, done, info = env.step(int(action),True)\n",
    "            new_state, reward, done, info = env.step(int(action))\n",
    "\n",
    "            print(\"state\",state,\"ne_state\",new_state,\"action\",action) \n",
    "            total_rewards += reward\n",
    "            env.render()\n",
    "            if done:    \n",
    "                #rewards.append(total_rewards)\n",
    "                print (\"Score\", total_rewards)\n",
    "                break\n",
    "            state = new_state\n",
    "    env.close()\n",
    "print (\"Score over time: \" ,  total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
